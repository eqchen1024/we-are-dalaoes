{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"twitter disaster.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"15Qz8r9K69LRuot6slSjatFTeHKttG70K","authorship_tag":"ABX9TyMoKNYeBOp6x/jnLgE2Cgag"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"6QcuXS669G0B","colab_type":"text"},"source":["装包"]},{"cell_type":"code","metadata":{"id":"QeVotnbSOGCt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":33},"executionInfo":{"status":"ok","timestamp":1596592038269,"user_tz":-480,"elapsed":1501,"user":{"displayName":"dy c","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiwUhiP2JGBJF0ccdzeCMO3DywIn5WL-okr917K=s64","userId":"07027550308680493425"}},"outputId":"8e4b6ab4-dd09-47fa-ef99-c7726577f95b"},"source":["%tensorflow_version 1.x"],"execution_count":1,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"K7Sd1eezOc-O","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1596592077699,"user_tz":-480,"elapsed":40913,"user":{"displayName":"dy c","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiwUhiP2JGBJF0ccdzeCMO3DywIn5WL-okr917K=s64","userId":"07027550308680493425"}},"outputId":"2828a352-c5fb-42c0-fb80-e4361d648a33"},"source":["!pip install gast==0.2.2\n","!pip install folium==0.2.1\n","!pip install imgaug==0.2.5\n","!pip install kashgari==1.1.0"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting gast==0.2.2\n","  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n","Building wheels for collected packages: gast\n","  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=7c71c474147168bfd1029cae9f44062c4070c2c65766cf17c6002c79f8677eca\n","  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n","Successfully built gast\n","Installing collected packages: gast\n","  Found existing installation: gast 0.3.3\n","    Uninstalling gast-0.3.3:\n","      Successfully uninstalled gast-0.3.3\n","Successfully installed gast-0.2.2\n","Collecting folium==0.2.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/dd/75ced7437bfa7cb9a88b96ee0177953062803c3b4cde411a97d98c35adaf/folium-0.2.1.tar.gz (69kB)\n","\u001b[K     |████████████████████████████████| 71kB 1.8MB/s \n","\u001b[?25hRequirement already satisfied: Jinja2 in /usr/local/lib/python3.6/dist-packages (from folium==0.2.1) (2.11.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2->folium==0.2.1) (1.1.1)\n","Building wheels for collected packages: folium\n","  Building wheel for folium (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for folium: filename=folium-0.2.1-cp36-none-any.whl size=79979 sha256=29f336dd68094acdfad6b9801a543fcf090d9c2e0423229f5c7342cb2a7f75a9\n","  Stored in directory: /root/.cache/pip/wheels/b8/09/f0/52d2ef419c2aaf4fb149f92a33e0008bdce7ae816f0dd8f0c5\n","Successfully built folium\n","Installing collected packages: folium\n","  Found existing installation: folium 0.8.3\n","    Uninstalling folium-0.8.3:\n","      Successfully uninstalled folium-0.8.3\n","Successfully installed folium-0.2.1\n","Collecting imgaug==0.2.5\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d2/60/a06a48d85a7e9062f5870347a3e3e953da30b37928d43b380c949bca458a/imgaug-0.2.5.tar.gz (562kB)\n","\u001b[K     |████████████████████████████████| 563kB 2.7MB/s \n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from imgaug==0.2.5) (1.4.1)\n","Requirement already satisfied: scikit-image>=0.11.0 in /usr/local/lib/python3.6/dist-packages (from imgaug==0.2.5) (0.16.2)\n","Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from imgaug==0.2.5) (1.18.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from imgaug==0.2.5) (1.15.0)\n","Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.5) (3.2.2)\n","Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.5) (1.1.1)\n","Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.5) (7.0.0)\n","Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.5) (2.4.1)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.5) (2.4)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug==0.2.5) (1.2.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug==0.2.5) (2.8.1)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug==0.2.5) (2.4.7)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug==0.2.5) (0.10.0)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.11.0->imgaug==0.2.5) (4.4.2)\n","Building wheels for collected packages: imgaug\n","  Building wheel for imgaug (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for imgaug: filename=imgaug-0.2.5-cp36-none-any.whl size=561439 sha256=af62c91383ef1e5f8f4188fe0e92131af56028367f0a20d58ffb282193a27445\n","  Stored in directory: /root/.cache/pip/wheels/31/48/c8/ca3345e8582a078de94243996e148377ef66fdb845557bae0b\n","Successfully built imgaug\n","Installing collected packages: imgaug\n","  Found existing installation: imgaug 0.2.9\n","    Uninstalling imgaug-0.2.9:\n","      Successfully uninstalled imgaug-0.2.9\n","Successfully installed imgaug-0.2.5\n","Collecting kashgari==1.1.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4f/bd/22158dacbc97e412e9c12d8745f0af31c2993f0326eb5413b2e35fe405d4/kashgari-1.1.0-py3-none-any.whl (84kB)\n","\u001b[K     |████████████████████████████████| 92kB 2.5MB/s \n","\u001b[?25hCollecting keras-gpt-2>=0.8.0\n","  Downloading https://files.pythonhosted.org/packages/62/31/4a838fff27a4fb7e4455e7848b1db4bfe7f8a4f5241c6012164c596e7d46/keras-gpt-2-0.15.0.tar.gz\n","Collecting numpy==1.16.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/2d/e4656149cbadd3a8a0369fcd1a9c7d61cc7b87b3903b85389c70c989a696/numpy-1.16.4-cp36-cp36m-manylinux1_x86_64.whl (17.3MB)\n","\u001b[K     |████████████████████████████████| 17.3MB 199kB/s \n","\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from kashgari==1.1.0) (2.10.0)\n","Requirement already satisfied: gensim>=3.5.0 in /usr/local/lib/python3.6/dist-packages (from kashgari==1.1.0) (3.6.0)\n","Collecting seqeval==0.0.10\n","  Downloading https://files.pythonhosted.org/packages/55/dd/3bf1c646c310daabae47fceb84ea9ab66df7f518a31a89955290d82b8100/seqeval-0.0.10-py3-none-any.whl\n","Requirement already satisfied: pandas>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from kashgari==1.1.0) (1.0.5)\n","Requirement already satisfied: scikit-learn>=0.21.1 in /usr/local/lib/python3.6/dist-packages (from kashgari==1.1.0) (0.22.2.post1)\n","Collecting keras-bert>=0.50.0\n","  Downloading https://files.pythonhosted.org/packages/e2/7f/95fabd29f4502924fa3f09ff6538c5a7d290dfef2c2fe076d3d1a16e08f0/keras-bert-0.86.0.tar.gz\n","Requirement already satisfied: Keras in /tensorflow-1.15.2/python3.6 (from keras-gpt-2>=0.8.0->kashgari==1.1.0) (2.3.1)\n","Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from keras-gpt-2>=0.8.0->kashgari==1.1.0) (2019.12.20)\n","Collecting keras-transformer>=0.38.0\n","  Downloading https://files.pythonhosted.org/packages/89/6c/d6f0c164f4cc16fbc0d0fea85f5526e87a7d2df7b077809e422a7e626150/keras-transformer-0.38.0.tar.gz\n","Collecting keras-pos-embd>=0.11.0\n","  Downloading https://files.pythonhosted.org/packages/09/70/b63ed8fc660da2bb6ae29b9895401c628da5740c048c190b5d7107cadd02/keras-pos-embd-0.11.0.tar.gz\n","Collecting keras-multi-head>=0.27.0\n","  Downloading https://files.pythonhosted.org/packages/e6/32/45adf2549450aca7867deccfa04af80a0ab1ca139af44b16bc669e0e09cd/keras-multi-head-0.27.0.tar.gz\n","Collecting keras-layer-normalization>=0.14.0\n","  Downloading https://files.pythonhosted.org/packages/a4/0e/d1078df0494bac9ce1a67954e5380b6e7569668f0f3b50a9531c62c1fc4a/keras-layer-normalization-0.14.0.tar.gz\n","Collecting keras-position-wise-feed-forward>=0.6.0\n","  Downloading https://files.pythonhosted.org/packages/e3/59/f0faa1037c033059e7e9e7758e6c23b4d1c0772cd48de14c4b6fd4033ad5/keras-position-wise-feed-forward-0.6.0.tar.gz\n","Collecting keras-embed-sim>=0.8.0\n","  Downloading https://files.pythonhosted.org/packages/57/ef/61a1e39082c9e1834a2d09261d4a0b69f7c818b359216d4e1912b20b1c86/keras-embed-sim-0.8.0.tar.gz\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.5.0->kashgari==1.1.0) (2.23.0)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.5.0->kashgari==1.1.0) (1.14.33)\n","Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.5.0->kashgari==1.1.0) (2.49.0)\n","Collecting keras-self-attention==0.46.0\n","  Downloading https://files.pythonhosted.org/packages/15/6b/c804924a056955fa1f3ff767945187103cfc851ba9bd0fc5a6c6bc18e2eb/keras-self-attention-0.46.0.tar.gz\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim>=3.5.0->kashgari==1.1.0) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim>=3.5.0->kashgari==1.1.0) (2020.6.20)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim>=3.5.0->kashgari==1.1.0) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim>=3.5.0->kashgari==1.1.0) (2.10)\n","Requirement already satisfied: botocore<1.18.0,>=1.17.33 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3.5.0->kashgari==1.1.0) (1.17.33)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3.5.0->kashgari==1.1.0) (0.10.0)\n","Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3.5.0->kashgari==1.1.0) (0.3.3)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.33->boto3->smart-open>=1.2.1->gensim>=3.5.0->kashgari==1.1.0) (0.15.2)\n","Building wheels for collected packages: keras-gpt-2, keras-bert, keras-transformer, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-self-attention\n","  Building wheel for keras-gpt-2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-gpt-2: filename=keras_gpt_2-0.15.0-cp36-none-any.whl size=10526 sha256=f6fc34464ec65ac551cad5fa040536dbd9e4b65c3a1aaf4415c4797c15bf6c85\n","  Stored in directory: /root/.cache/pip/wheels/c5/b3/c7/fb39496446fc6493380fe3983788cc2dae979acdf3d00ee97f\n","  Building wheel for keras-bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-bert: filename=keras_bert-0.86.0-cp36-none-any.whl size=34143 sha256=a0a6c70805613534e95f43d5056d34926dc5470f5f511d8a6b33b335b3c19bc2\n","  Stored in directory: /root/.cache/pip/wheels/66/f0/b1/748128b58562fc9e31b907bb5e2ab6a35eb37695e83911236b\n","  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-transformer: filename=keras_transformer-0.38.0-cp36-none-any.whl size=12942 sha256=d4b61b9d0332daa2b97be1af1b2128e3350f14a6f24d35db03866d139809992e\n","  Stored in directory: /root/.cache/pip/wheels/e5/fb/3a/37b2b9326c799aa010ae46a04ddb04f320d8c77c0b7e837f4e\n","  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.11.0-cp36-none-any.whl size=7554 sha256=6806d2d51c5c818c74dbb8c0a8e2897a481cfedbee3010528ce5cac2e19af65d\n","  Stored in directory: /root/.cache/pip/wheels/5b/a1/a0/ce6b1d49ba1a9a76f592e70cf297b05c96bc9f418146761032\n","  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-multi-head: filename=keras_multi_head-0.27.0-cp36-none-any.whl size=15611 sha256=4f8ffe2e628193a72bff6c1bede28089e4242c0f20f2fdd7a62e77f0ad694dbb\n","  Stored in directory: /root/.cache/pip/wheels/b5/b4/49/0a0c27dcb93c13af02fea254ff51d1a43a924dd4e5b7a7164d\n","  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.14.0-cp36-none-any.whl size=5268 sha256=be319d65f6e7d4e543e561b0f0117d46994c94d9b6b211c076cd9b3c200576be\n","  Stored in directory: /root/.cache/pip/wheels/54/80/22/a638a7d406fd155e507aa33d703e3fa2612b9eb7bb4f4fe667\n","  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.6.0-cp36-none-any.whl size=5623 sha256=2f034e7e3975fc4d5bd4f8e1e4f3323ad82afbf2432c1d13b038c19ea139ec29\n","  Stored in directory: /root/.cache/pip/wheels/39/e2/e2/3514fef126a00574b13bc0b9e23891800158df3a3c19c96e3b\n","  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.8.0-cp36-none-any.whl size=4558 sha256=f41bd18d91f56b68faf9609bd95853c1f30e0c5e8b9d4b9392a6540a61382e30\n","  Stored in directory: /root/.cache/pip/wheels/49/45/8b/c111f6cc8bec253e984677de73a6f4f5d2f1649f42aac191c8\n","  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-self-attention: filename=keras_self_attention-0.46.0-cp36-none-any.whl size=17278 sha256=8485f80018c0d522b8a09cf31661e010ab857a365fbe3f8114cd4c32e61119b4\n","  Stored in directory: /root/.cache/pip/wheels/d2/2e/80/fec4c05eb23c8e13b790e26d207d6e0ffe8013fad8c6bdd4d2\n","Successfully built keras-gpt-2 keras-bert keras-transformer keras-pos-embd keras-multi-head keras-layer-normalization keras-position-wise-feed-forward keras-embed-sim keras-self-attention\n","\u001b[31mERROR: umap-learn 0.4.6 has requirement numpy>=1.17, but you'll have numpy 1.16.4 which is incompatible.\u001b[0m\n","\u001b[31mERROR: keras-bert 0.86.0 has requirement Keras>=2.4.3, but you'll have keras 2.3.1 which is incompatible.\u001b[0m\n","Installing collected packages: numpy, keras-pos-embd, keras-self-attention, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-transformer, keras-gpt-2, seqeval, keras-bert, kashgari\n","  Found existing installation: numpy 1.18.5\n","    Uninstalling numpy-1.18.5:\n","      Successfully uninstalled numpy-1.18.5\n","Successfully installed kashgari-1.1.0 keras-bert-0.86.0 keras-embed-sim-0.8.0 keras-gpt-2-0.15.0 keras-layer-normalization-0.14.0 keras-multi-head-0.27.0 keras-pos-embd-0.11.0 keras-position-wise-feed-forward-0.6.0 keras-self-attention-0.46.0 keras-transformer-0.38.0 numpy-1.16.4 seqeval-0.0.10\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy"]}}},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"Sfif231OOj5U","colab_type":"text"},"source":["数据加载 训练测试集划分"]},{"cell_type":"code","metadata":{"id":"7TwXJIVd4MKe","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596592079068,"user_tz":-480,"elapsed":42278,"user":{"displayName":"dy c","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiwUhiP2JGBJF0ccdzeCMO3DywIn5WL-okr917K=s64","userId":"07027550308680493425"}}},"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ixowz89o4bE7","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596592084145,"user_tz":-480,"elapsed":47351,"user":{"displayName":"dy c","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiwUhiP2JGBJF0ccdzeCMO3DywIn5WL-okr917K=s64","userId":"07027550308680493425"}}},"source":["df=pd.read_csv(r\"/content/drive/My Drive/KAGGLE/nlp/twitter/data/train.csv\")"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"7recyvWjjfbc","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596592084147,"user_tz":-480,"elapsed":47348,"user":{"displayName":"dy c","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiwUhiP2JGBJF0ccdzeCMO3DywIn5WL-okr917K=s64","userId":"07027550308680493425"}}},"source":["df=df.fillna(\" \")"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"JxheTiUsiU3G","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596592084148,"user_tz":-480,"elapsed":47344,"user":{"displayName":"dy c","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiwUhiP2JGBJF0ccdzeCMO3DywIn5WL-okr917K=s64","userId":"07027550308680493425"}}},"source":["import re\n","import string\n","def pre(x):\n","    x = re.sub('\\[.*?\\]', '', x)\n","    x = re.sub('https?://\\S+|www\\.\\S+', '', x)\n","    x = re.sub('<.*?>+', '', x)\n","    x = re.sub('[%s]' % re.escape(string.punctuation), '', x)\n","    x = re.sub('\\n', '', x)\n","    x = re.sub('\\w*\\d\\w*', '', x)\n","    x = x.lower()\n","    #raw_x[i]=correct_spellings(raw_x[i])\n","    return x"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"kiK85EZciATL","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596592084149,"user_tz":-480,"elapsed":47342,"user":{"displayName":"dy c","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiwUhiP2JGBJF0ccdzeCMO3DywIn5WL-okr917K=s64","userId":"07027550308680493425"}}},"source":["cleaned_txt=df[\"text\"].apply(pre)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"iZxsif4ejaVO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":216},"executionInfo":{"status":"ok","timestamp":1596592084149,"user_tz":-480,"elapsed":47331,"user":{"displayName":"dy c","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiwUhiP2JGBJF0ccdzeCMO3DywIn5WL-okr917K=s64","userId":"07027550308680493425"}},"outputId":"a86ac1a8-6faf-4630-abc0-d1a36694fb51"},"source":["new_feature=\"$ \"+df['keyword']+\" $ \"+df[\"location\"]+\" $ \"+cleaned_txt\n","print(new_feature.head(100))"],"execution_count":8,"outputs":[{"output_type":"stream","text":["0     $   $   $ our deeds are the reason of this ear...\n","1       $   $   $ forest fire near la ronge sask canada\n","2     $   $   $ all residents asked to shelter in pl...\n","3     $   $   $  people receive wildfires evacuation...\n","4     $   $   $ just got sent this photo from ruby a...\n","                            ...                        \n","95    $ accident $ Charlotte $  mile backup on  sout...\n","96    $ accident $ Baton Rouge, LA $ has an accident...\n","97    $ accident $ Hagerstown, MD $ breaking there w...\n","98    $ accident $ Gloucestershire , UK $ flowri wer...\n","99    $ accident $   $ only had a car for not even a...\n","Length: 100, dtype: object\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qCtwszSNZFzJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":67},"executionInfo":{"status":"ok","timestamp":1596592084150,"user_tz":-480,"elapsed":47323,"user":{"displayName":"dy c","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiwUhiP2JGBJF0ccdzeCMO3DywIn5WL-okr917K=s64","userId":"07027550308680493425"}},"outputId":"34e9223f-8d6f-49d7-b1fe-9ffdd7a7ea03"},"source":["'''\n","import re\n","import string\n","def preprocessing(raw_x):\n","    for i in range(len(raw_x)):\n","        raw_x[i]=re.sub('\\[.*?\\]', '', raw_x[i])\n","        raw_x[i] = re.sub('https?://\\S+|www\\.\\S+', '', raw_x[i])\n","        raw_x[i] = re.sub('<.*?>+', '', raw_x[i])\n","        raw_x[i] = re.sub('[%s]' % re.escape(string.punctuation), '', raw_x[i])\n","        raw_x[i] = re.sub('\\n', '', raw_x[i])\n","        raw_x[i] = re.sub('\\w*\\d\\w*', '', raw_x[i])\n","        raw_x[i] = raw_x[i].lower()\n","        #raw_x[i]=correct_spellings(raw_x[i])\n","    return raw_x\n","'''"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"\\nimport re\\nimport string\\ndef preprocessing(raw_x):\\n    for i in range(len(raw_x)):\\n        raw_x[i]=re.sub('\\\\[.*?\\\\]', '', raw_x[i])\\n        raw_x[i] = re.sub('https?://\\\\S+|www\\\\.\\\\S+', '', raw_x[i])\\n        raw_x[i] = re.sub('<.*?>+', '', raw_x[i])\\n        raw_x[i] = re.sub('[%s]' % re.escape(string.punctuation), '', raw_x[i])\\n        raw_x[i] = re.sub('\\n', '', raw_x[i])\\n        raw_x[i] = re.sub('\\\\w*\\\\d\\\\w*', '', raw_x[i])\\n        raw_x[i] = raw_x[i].lower()\\n        #raw_x[i]=correct_spellings(raw_x[i])\\n    return raw_x\\n\""]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"pZfQhSPf4mHB","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596592084151,"user_tz":-480,"elapsed":47317,"user":{"displayName":"dy c","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiwUhiP2JGBJF0ccdzeCMO3DywIn5WL-okr917K=s64","userId":"07027550308680493425"}}},"source":["raw_x=new_feature.values.tolist()\n","raw_y=df[\"target\"].values.tolist()\n","raw_data=list(zip(raw_x,raw_y))"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"FQjJYrgnPWXP","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596592084152,"user_tz":-480,"elapsed":47313,"user":{"displayName":"dy c","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiwUhiP2JGBJF0ccdzeCMO3DywIn5WL-okr917K=s64","userId":"07027550308680493425"}}},"source":["training_data,test_data=train_test_split(raw_data,test_size=0.2, random_state=16)\n","training_data_x=[[w for w in sample[0].split()] for sample in training_data]\n","training_data_y=[sample[1] for sample in training_data]\n","test_data_x=[[w for w in sample[0].split()] for sample in test_data]\n","test_data_y=[sample[1] for sample in test_data]"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vw1bRjHOSVKS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"executionInfo":{"status":"ok","timestamp":1596592090089,"user_tz":-480,"elapsed":53237,"user":{"displayName":"dy c","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiwUhiP2JGBJF0ccdzeCMO3DywIn5WL-okr917K=s64","userId":"07027550308680493425"}},"outputId":"13ea75ab-3549-461d-fa81-2327253ba013"},"source":["import kashgari\n","from kashgari.embeddings import BERTEmbedding\n","kashgari.config.use_cudnn_cell = True"],"execution_count":12,"outputs":[{"output_type":"stream","text":["WARNING:root:CUDA GPU available, you can set `kashgari.config.use_cudnn_cell = True` to use CuDNNCell. This will speed up the training, but will make model incompatible with CPU device.\n","WARNING:root:CuDNN enabled, this will speed up the training, but will make model incompatible with CPU device.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"dBwr414CSdRV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":33},"executionInfo":{"status":"ok","timestamp":1596592225041,"user_tz":-480,"elapsed":188178,"user":{"displayName":"dy c","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiwUhiP2JGBJF0ccdzeCMO3DywIn5WL-okr917K=s64","userId":"07027550308680493425"}},"outputId":"9570e308-dd46-4d96-9002-40d4ce2188ff"},"source":["bert_embed = BERTEmbedding('/content/drive/My Drive/uncased_L-12_H-768_A-12/',\n","                           task=kashgari.CLASSIFICATION,\n","                           sequence_length=30)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["WARNING:root:seq_len: 30\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"vZL1wmlG-DL9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1596593799347,"user_tz":-480,"elapsed":1406002,"user":{"displayName":"dy c","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiwUhiP2JGBJF0ccdzeCMO3DywIn5WL-okr917K=s64","userId":"07027550308680493425"}},"outputId":"36e269fa-52f7-4527-9616-554023d6d130"},"source":["from tensorflow.python import keras\n","import tensorflow as tf\n","import kashgari\n","from kashgari.tasks.classification import BiLSTM_Model\n","from kashgari.callbacks import EvalCallBack\n","import logging\n","logging.basicConfig(level='DEBUG')\n","model = BiLSTM_Model(bert_embed)\n","eval_callback = EvalCallBack(kash_model=model,valid_x=test_data_x,valid_y=test_data_y,step=1)\n","model.fit(training_data_x,\n","          training_data_y,\n","          test_data_x,\n","          test_data_y,\n","          batch_size=32,epochs=100,callbacks=[eval_callback])"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Model: \"model_4\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","Input-Token (InputLayer)        [(None, 30)]         0                                            \n","__________________________________________________________________________________________________\n","Input-Segment (InputLayer)      [(None, 30)]         0                                            \n","__________________________________________________________________________________________________\n","Embedding-Token (TokenEmbedding [(None, 30, 768), (3 23440896    Input-Token[0][0]                \n","__________________________________________________________________________________________________\n","Embedding-Segment (Embedding)   (None, 30, 768)      1536        Input-Segment[0][0]              \n","__________________________________________________________________________________________________\n","Embedding-Token-Segment (Add)   (None, 30, 768)      0           Embedding-Token[0][0]            \n","                                                                 Embedding-Segment[0][0]          \n","__________________________________________________________________________________________________\n","Embedding-Position (PositionEmb (None, 30, 768)      23040       Embedding-Token-Segment[0][0]    \n","__________________________________________________________________________________________________\n","Embedding-Dropout (Dropout)     (None, 30, 768)      0           Embedding-Position[0][0]         \n","__________________________________________________________________________________________________\n","Embedding-Norm (LayerNormalizat (None, 30, 768)      1536        Embedding-Dropout[0][0]          \n","__________________________________________________________________________________________________\n","Encoder-1-MultiHeadSelfAttentio (None, 30, 768)      2362368     Embedding-Norm[0][0]             \n","__________________________________________________________________________________________________\n","Encoder-1-MultiHeadSelfAttentio (None, 30, 768)      0           Encoder-1-MultiHeadSelfAttention[\n","__________________________________________________________________________________________________\n","Encoder-1-MultiHeadSelfAttentio (None, 30, 768)      0           Embedding-Norm[0][0]             \n","                                                                 Encoder-1-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-1-MultiHeadSelfAttentio (None, 30, 768)      1536        Encoder-1-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-1-FeedForward (FeedForw (None, 30, 768)      4722432     Encoder-1-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-1-FeedForward-Dropout ( (None, 30, 768)      0           Encoder-1-FeedForward[0][0]      \n","__________________________________________________________________________________________________\n","Encoder-1-FeedForward-Add (Add) (None, 30, 768)      0           Encoder-1-MultiHeadSelfAttention-\n","                                                                 Encoder-1-FeedForward-Dropout[0][\n","__________________________________________________________________________________________________\n","Encoder-1-FeedForward-Norm (Lay (None, 30, 768)      1536        Encoder-1-FeedForward-Add[0][0]  \n","__________________________________________________________________________________________________\n","Encoder-2-MultiHeadSelfAttentio (None, 30, 768)      2362368     Encoder-1-FeedForward-Norm[0][0] \n","__________________________________________________________________________________________________\n","Encoder-2-MultiHeadSelfAttentio (None, 30, 768)      0           Encoder-2-MultiHeadSelfAttention[\n","__________________________________________________________________________________________________\n","Encoder-2-MultiHeadSelfAttentio (None, 30, 768)      0           Encoder-1-FeedForward-Norm[0][0] \n","                                                                 Encoder-2-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-2-MultiHeadSelfAttentio (None, 30, 768)      1536        Encoder-2-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-2-FeedForward (FeedForw (None, 30, 768)      4722432     Encoder-2-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-2-FeedForward-Dropout ( (None, 30, 768)      0           Encoder-2-FeedForward[0][0]      \n","__________________________________________________________________________________________________\n","Encoder-2-FeedForward-Add (Add) (None, 30, 768)      0           Encoder-2-MultiHeadSelfAttention-\n","                                                                 Encoder-2-FeedForward-Dropout[0][\n","__________________________________________________________________________________________________\n","Encoder-2-FeedForward-Norm (Lay (None, 30, 768)      1536        Encoder-2-FeedForward-Add[0][0]  \n","__________________________________________________________________________________________________\n","Encoder-3-MultiHeadSelfAttentio (None, 30, 768)      2362368     Encoder-2-FeedForward-Norm[0][0] \n","__________________________________________________________________________________________________\n","Encoder-3-MultiHeadSelfAttentio (None, 30, 768)      0           Encoder-3-MultiHeadSelfAttention[\n","__________________________________________________________________________________________________\n","Encoder-3-MultiHeadSelfAttentio (None, 30, 768)      0           Encoder-2-FeedForward-Norm[0][0] \n","                                                                 Encoder-3-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-3-MultiHeadSelfAttentio (None, 30, 768)      1536        Encoder-3-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-3-FeedForward (FeedForw (None, 30, 768)      4722432     Encoder-3-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-3-FeedForward-Dropout ( (None, 30, 768)      0           Encoder-3-FeedForward[0][0]      \n","__________________________________________________________________________________________________\n","Encoder-3-FeedForward-Add (Add) (None, 30, 768)      0           Encoder-3-MultiHeadSelfAttention-\n","                                                                 Encoder-3-FeedForward-Dropout[0][\n","__________________________________________________________________________________________________\n","Encoder-3-FeedForward-Norm (Lay (None, 30, 768)      1536        Encoder-3-FeedForward-Add[0][0]  \n","__________________________________________________________________________________________________\n","Encoder-4-MultiHeadSelfAttentio (None, 30, 768)      2362368     Encoder-3-FeedForward-Norm[0][0] \n","__________________________________________________________________________________________________\n","Encoder-4-MultiHeadSelfAttentio (None, 30, 768)      0           Encoder-4-MultiHeadSelfAttention[\n","__________________________________________________________________________________________________\n","Encoder-4-MultiHeadSelfAttentio (None, 30, 768)      0           Encoder-3-FeedForward-Norm[0][0] \n","                                                                 Encoder-4-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-4-MultiHeadSelfAttentio (None, 30, 768)      1536        Encoder-4-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-4-FeedForward (FeedForw (None, 30, 768)      4722432     Encoder-4-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-4-FeedForward-Dropout ( (None, 30, 768)      0           Encoder-4-FeedForward[0][0]      \n","__________________________________________________________________________________________________\n","Encoder-4-FeedForward-Add (Add) (None, 30, 768)      0           Encoder-4-MultiHeadSelfAttention-\n","                                                                 Encoder-4-FeedForward-Dropout[0][\n","__________________________________________________________________________________________________\n","Encoder-4-FeedForward-Norm (Lay (None, 30, 768)      1536        Encoder-4-FeedForward-Add[0][0]  \n","__________________________________________________________________________________________________\n","Encoder-5-MultiHeadSelfAttentio (None, 30, 768)      2362368     Encoder-4-FeedForward-Norm[0][0] \n","__________________________________________________________________________________________________\n","Encoder-5-MultiHeadSelfAttentio (None, 30, 768)      0           Encoder-5-MultiHeadSelfAttention[\n","__________________________________________________________________________________________________\n","Encoder-5-MultiHeadSelfAttentio (None, 30, 768)      0           Encoder-4-FeedForward-Norm[0][0] \n","                                                                 Encoder-5-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-5-MultiHeadSelfAttentio (None, 30, 768)      1536        Encoder-5-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-5-FeedForward (FeedForw (None, 30, 768)      4722432     Encoder-5-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-5-FeedForward-Dropout ( (None, 30, 768)      0           Encoder-5-FeedForward[0][0]      \n","__________________________________________________________________________________________________\n","Encoder-5-FeedForward-Add (Add) (None, 30, 768)      0           Encoder-5-MultiHeadSelfAttention-\n","                                                                 Encoder-5-FeedForward-Dropout[0][\n","__________________________________________________________________________________________________\n","Encoder-5-FeedForward-Norm (Lay (None, 30, 768)      1536        Encoder-5-FeedForward-Add[0][0]  \n","__________________________________________________________________________________________________\n","Encoder-6-MultiHeadSelfAttentio (None, 30, 768)      2362368     Encoder-5-FeedForward-Norm[0][0] \n","__________________________________________________________________________________________________\n","Encoder-6-MultiHeadSelfAttentio (None, 30, 768)      0           Encoder-6-MultiHeadSelfAttention[\n","__________________________________________________________________________________________________\n","Encoder-6-MultiHeadSelfAttentio (None, 30, 768)      0           Encoder-5-FeedForward-Norm[0][0] \n","                                                                 Encoder-6-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-6-MultiHeadSelfAttentio (None, 30, 768)      1536        Encoder-6-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-6-FeedForward (FeedForw (None, 30, 768)      4722432     Encoder-6-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-6-FeedForward-Dropout ( (None, 30, 768)      0           Encoder-6-FeedForward[0][0]      \n","__________________________________________________________________________________________________\n","Encoder-6-FeedForward-Add (Add) (None, 30, 768)      0           Encoder-6-MultiHeadSelfAttention-\n","                                                                 Encoder-6-FeedForward-Dropout[0][\n","__________________________________________________________________________________________________\n","Encoder-6-FeedForward-Norm (Lay (None, 30, 768)      1536        Encoder-6-FeedForward-Add[0][0]  \n","__________________________________________________________________________________________________\n","Encoder-7-MultiHeadSelfAttentio (None, 30, 768)      2362368     Encoder-6-FeedForward-Norm[0][0] \n","__________________________________________________________________________________________________\n","Encoder-7-MultiHeadSelfAttentio (None, 30, 768)      0           Encoder-7-MultiHeadSelfAttention[\n","__________________________________________________________________________________________________\n","Encoder-7-MultiHeadSelfAttentio (None, 30, 768)      0           Encoder-6-FeedForward-Norm[0][0] \n","                                                                 Encoder-7-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-7-MultiHeadSelfAttentio (None, 30, 768)      1536        Encoder-7-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-7-FeedForward (FeedForw (None, 30, 768)      4722432     Encoder-7-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-7-FeedForward-Dropout ( (None, 30, 768)      0           Encoder-7-FeedForward[0][0]      \n","__________________________________________________________________________________________________\n","Encoder-7-FeedForward-Add (Add) (None, 30, 768)      0           Encoder-7-MultiHeadSelfAttention-\n","                                                                 Encoder-7-FeedForward-Dropout[0][\n","__________________________________________________________________________________________________\n","Encoder-7-FeedForward-Norm (Lay (None, 30, 768)      1536        Encoder-7-FeedForward-Add[0][0]  \n","__________________________________________________________________________________________________\n","Encoder-8-MultiHeadSelfAttentio (None, 30, 768)      2362368     Encoder-7-FeedForward-Norm[0][0] \n","__________________________________________________________________________________________________\n","Encoder-8-MultiHeadSelfAttentio (None, 30, 768)      0           Encoder-8-MultiHeadSelfAttention[\n","__________________________________________________________________________________________________\n","Encoder-8-MultiHeadSelfAttentio (None, 30, 768)      0           Encoder-7-FeedForward-Norm[0][0] \n","                                                                 Encoder-8-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-8-MultiHeadSelfAttentio (None, 30, 768)      1536        Encoder-8-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-8-FeedForward (FeedForw (None, 30, 768)      4722432     Encoder-8-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-8-FeedForward-Dropout ( (None, 30, 768)      0           Encoder-8-FeedForward[0][0]      \n","__________________________________________________________________________________________________\n","Encoder-8-FeedForward-Add (Add) (None, 30, 768)      0           Encoder-8-MultiHeadSelfAttention-\n","                                                                 Encoder-8-FeedForward-Dropout[0][\n","__________________________________________________________________________________________________\n","Encoder-8-FeedForward-Norm (Lay (None, 30, 768)      1536        Encoder-8-FeedForward-Add[0][0]  \n","__________________________________________________________________________________________________\n","Encoder-9-MultiHeadSelfAttentio (None, 30, 768)      2362368     Encoder-8-FeedForward-Norm[0][0] \n","__________________________________________________________________________________________________\n","Encoder-9-MultiHeadSelfAttentio (None, 30, 768)      0           Encoder-9-MultiHeadSelfAttention[\n","__________________________________________________________________________________________________\n","Encoder-9-MultiHeadSelfAttentio (None, 30, 768)      0           Encoder-8-FeedForward-Norm[0][0] \n","                                                                 Encoder-9-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-9-MultiHeadSelfAttentio (None, 30, 768)      1536        Encoder-9-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-9-FeedForward (FeedForw (None, 30, 768)      4722432     Encoder-9-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-9-FeedForward-Dropout ( (None, 30, 768)      0           Encoder-9-FeedForward[0][0]      \n","__________________________________________________________________________________________________\n","Encoder-9-FeedForward-Add (Add) (None, 30, 768)      0           Encoder-9-MultiHeadSelfAttention-\n","                                                                 Encoder-9-FeedForward-Dropout[0][\n","__________________________________________________________________________________________________\n","Encoder-9-FeedForward-Norm (Lay (None, 30, 768)      1536        Encoder-9-FeedForward-Add[0][0]  \n","__________________________________________________________________________________________________\n","Encoder-10-MultiHeadSelfAttenti (None, 30, 768)      2362368     Encoder-9-FeedForward-Norm[0][0] \n","__________________________________________________________________________________________________\n","Encoder-10-MultiHeadSelfAttenti (None, 30, 768)      0           Encoder-10-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-10-MultiHeadSelfAttenti (None, 30, 768)      0           Encoder-9-FeedForward-Norm[0][0] \n","                                                                 Encoder-10-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-10-MultiHeadSelfAttenti (None, 30, 768)      1536        Encoder-10-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-10-FeedForward (FeedFor (None, 30, 768)      4722432     Encoder-10-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-10-FeedForward-Dropout  (None, 30, 768)      0           Encoder-10-FeedForward[0][0]     \n","__________________________________________________________________________________________________\n","Encoder-10-FeedForward-Add (Add (None, 30, 768)      0           Encoder-10-MultiHeadSelfAttention\n","                                                                 Encoder-10-FeedForward-Dropout[0]\n","__________________________________________________________________________________________________\n","Encoder-10-FeedForward-Norm (La (None, 30, 768)      1536        Encoder-10-FeedForward-Add[0][0] \n","__________________________________________________________________________________________________\n","Encoder-11-MultiHeadSelfAttenti (None, 30, 768)      2362368     Encoder-10-FeedForward-Norm[0][0]\n","__________________________________________________________________________________________________\n","Encoder-11-MultiHeadSelfAttenti (None, 30, 768)      0           Encoder-11-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-11-MultiHeadSelfAttenti (None, 30, 768)      0           Encoder-10-FeedForward-Norm[0][0]\n","                                                                 Encoder-11-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-11-MultiHeadSelfAttenti (None, 30, 768)      1536        Encoder-11-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-11-FeedForward (FeedFor (None, 30, 768)      4722432     Encoder-11-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-11-FeedForward-Dropout  (None, 30, 768)      0           Encoder-11-FeedForward[0][0]     \n","__________________________________________________________________________________________________\n","Encoder-11-FeedForward-Add (Add (None, 30, 768)      0           Encoder-11-MultiHeadSelfAttention\n","                                                                 Encoder-11-FeedForward-Dropout[0]\n","__________________________________________________________________________________________________\n","Encoder-11-FeedForward-Norm (La (None, 30, 768)      1536        Encoder-11-FeedForward-Add[0][0] \n","__________________________________________________________________________________________________\n","Encoder-12-MultiHeadSelfAttenti (None, 30, 768)      2362368     Encoder-11-FeedForward-Norm[0][0]\n","__________________________________________________________________________________________________\n","Encoder-12-MultiHeadSelfAttenti (None, 30, 768)      0           Encoder-12-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-12-MultiHeadSelfAttenti (None, 30, 768)      0           Encoder-11-FeedForward-Norm[0][0]\n","                                                                 Encoder-12-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-12-MultiHeadSelfAttenti (None, 30, 768)      1536        Encoder-12-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-12-FeedForward (FeedFor (None, 30, 768)      4722432     Encoder-12-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-12-FeedForward-Dropout  (None, 30, 768)      0           Encoder-12-FeedForward[0][0]     \n","__________________________________________________________________________________________________\n","Encoder-12-FeedForward-Add (Add (None, 30, 768)      0           Encoder-12-MultiHeadSelfAttention\n","                                                                 Encoder-12-FeedForward-Dropout[0]\n","__________________________________________________________________________________________________\n","Encoder-12-FeedForward-Norm (La (None, 30, 768)      1536        Encoder-12-FeedForward-Add[0][0] \n","__________________________________________________________________________________________________\n","Encoder-Output (Concatenate)    (None, 30, 3072)     0           Encoder-9-FeedForward-Norm[0][0] \n","                                                                 Encoder-10-FeedForward-Norm[0][0]\n","                                                                 Encoder-11-FeedForward-Norm[0][0]\n","                                                                 Encoder-12-FeedForward-Norm[0][0]\n","__________________________________________________________________________________________________\n","non_masking_layer (NonMaskingLa (None, 30, 3072)     0           Encoder-Output[0][0]             \n","__________________________________________________________________________________________________\n","bidirectional (Bidirectional)   (None, 256)          3278848     non_masking_layer[0][0]          \n","__________________________________________________________________________________________________\n","dense (Dense)                   (None, 2)            514         bidirectional[0][0]              \n","==================================================================================================\n","Total params: 111,800,834\n","Trainable params: 3,279,362\n","Non-trainable params: 108,521,472\n","__________________________________________________________________________________________________\n","Epoch 1/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.5565 - acc: 0.7431Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 15s - loss: 0.4567 - acc: 0.7945\n","epoch: 0 precision: 0.798075, recall: 0.794485, f1: 0.790573\n","191/191 [==============================] - 38s 198ms/step - loss: 0.5565 - acc: 0.7432 - val_loss: 0.4567 - val_acc: 0.7945\n","Epoch 2/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.4451 - acc: 0.8028Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.4277 - acc: 0.8056\n","epoch: 1 precision: 0.803296, recall: 0.801051, f1: 0.797999\n","191/191 [==============================] - 15s 81ms/step - loss: 0.4442 - acc: 0.8030 - val_loss: 0.4277 - val_acc: 0.8056\n","Epoch 3/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.4179 - acc: 0.8133Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.4336 - acc: 0.8063\n","epoch: 2 precision: 0.810848, recall: 0.808930, f1: 0.806292\n","191/191 [==============================] - 15s 81ms/step - loss: 0.4193 - acc: 0.8131 - val_loss: 0.4336 - val_acc: 0.8063\n","Epoch 4/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.3959 - acc: 0.8215Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.4359 - acc: 0.8194\n","epoch: 3 precision: 0.817386, recall: 0.816152, f1: 0.814048\n","191/191 [==============================] - 15s 81ms/step - loss: 0.3949 - acc: 0.8217 - val_loss: 0.4359 - val_acc: 0.8194\n","Epoch 5/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.3784 - acc: 0.8387Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.4671 - acc: 0.8017\n","epoch: 4 precision: 0.821254, recall: 0.802364, f1: 0.794146\n","191/191 [==============================] - 15s 81ms/step - loss: 0.3783 - acc: 0.8386 - val_loss: 0.4671 - val_acc: 0.8017\n","Epoch 6/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.3565 - acc: 0.8490Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.4529 - acc: 0.8096\n","epoch: 5 precision: 0.816035, recall: 0.813526, f1: 0.810775\n","191/191 [==============================] - 15s 81ms/step - loss: 0.3580 - acc: 0.8488 - val_loss: 0.4529 - val_acc: 0.8096\n","Epoch 7/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.3361 - acc: 0.8602Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.4249 - acc: 0.8181\n","epoch: 6 precision: 0.807971, recall: 0.808273, f1: 0.808087\n","191/191 [==============================] - 15s 81ms/step - loss: 0.3361 - acc: 0.8601 - val_loss: 0.4249 - val_acc: 0.8181\n","Epoch 8/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.3155 - acc: 0.8699Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.4544 - acc: 0.8221\n","epoch: 7 precision: 0.822183, recall: 0.820092, f1: 0.817709\n","191/191 [==============================] - 15s 81ms/step - loss: 0.3147 - acc: 0.8701 - val_loss: 0.4544 - val_acc: 0.8221\n","Epoch 9/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.2949 - acc: 0.8794Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.4609 - acc: 0.8056\n","epoch: 8 precision: 0.807805, recall: 0.808273, f1: 0.807215\n","191/191 [==============================] - 15s 81ms/step - loss: 0.2948 - acc: 0.8795 - val_loss: 0.4609 - val_acc: 0.8056\n","Epoch 10/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.2760 - acc: 0.8855Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.4826 - acc: 0.7978\n","epoch: 9 precision: 0.806374, recall: 0.806960, f1: 0.806389\n","191/191 [==============================] - 16s 83ms/step - loss: 0.2775 - acc: 0.8852 - val_loss: 0.4826 - val_acc: 0.7978\n","Epoch 11/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.2573 - acc: 0.8942Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.4709 - acc: 0.8181\n","epoch: 10 precision: 0.811624, recall: 0.810243, f1: 0.807903\n","191/191 [==============================] - 16s 82ms/step - loss: 0.2573 - acc: 0.8944 - val_loss: 0.4709 - val_acc: 0.8181\n","Epoch 12/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.2397 - acc: 0.9035Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.4688 - acc: 0.8116\n","epoch: 11 precision: 0.803379, recall: 0.803677, f1: 0.802285\n","191/191 [==============================] - 16s 81ms/step - loss: 0.2399 - acc: 0.9034 - val_loss: 0.4688 - val_acc: 0.8116\n","Epoch 13/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.2276 - acc: 0.9112Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.5091 - acc: 0.8063\n","epoch: 12 precision: 0.809554, recall: 0.809586, f1: 0.808093\n","191/191 [==============================] - 16s 82ms/step - loss: 0.2275 - acc: 0.9112 - val_loss: 0.5091 - val_acc: 0.8063\n","Epoch 14/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.2144 - acc: 0.9127Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.5122 - acc: 0.8063\n","epoch: 13 precision: 0.803133, recall: 0.803677, f1: 0.802620\n","191/191 [==============================] - 16s 81ms/step - loss: 0.2149 - acc: 0.9125 - val_loss: 0.5122 - val_acc: 0.8063\n","Epoch 15/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.1967 - acc: 0.9238Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.5107 - acc: 0.8109\n","epoch: 14 precision: 0.800819, recall: 0.801051, f1: 0.799520\n","191/191 [==============================] - 16s 81ms/step - loss: 0.1965 - acc: 0.9240 - val_loss: 0.5107 - val_acc: 0.8109\n","Epoch 16/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.1877 - acc: 0.9275Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.5689 - acc: 0.8122\n","epoch: 15 precision: 0.811236, recall: 0.810243, f1: 0.808104\n","191/191 [==============================] - 16s 81ms/step - loss: 0.1875 - acc: 0.9276 - val_loss: 0.5689 - val_acc: 0.8122\n","Epoch 17/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.1873 - acc: 0.9252Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.5154 - acc: 0.8056\n","epoch: 16 precision: 0.799481, recall: 0.799081, f1: 0.796988\n","191/191 [==============================] - 16s 81ms/step - loss: 0.1869 - acc: 0.9253 - val_loss: 0.5154 - val_acc: 0.8056\n","Epoch 18/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.1742 - acc: 0.9322Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.5877 - acc: 0.8011\n","epoch: 17 precision: 0.808445, recall: 0.803677, f1: 0.799684\n","191/191 [==============================] - 16s 82ms/step - loss: 0.1739 - acc: 0.9323 - val_loss: 0.5877 - val_acc: 0.8011\n","Epoch 19/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.1590 - acc: 0.9416Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.6074 - acc: 0.7899\n","epoch: 18 precision: 0.793654, recall: 0.793171, f1: 0.793371\n","191/191 [==============================] - 15s 81ms/step - loss: 0.1590 - acc: 0.9417 - val_loss: 0.6074 - val_acc: 0.7899\n","Epoch 20/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.1546 - acc: 0.9418Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.6084 - acc: 0.7840\n","epoch: 19 precision: 0.780057, recall: 0.777413, f1: 0.778115\n","191/191 [==============================] - 16s 81ms/step - loss: 0.1544 - acc: 0.9417 - val_loss: 0.6084 - val_acc: 0.7840\n","Epoch 21/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.1524 - acc: 0.9413Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.5859 - acc: 0.8168\n","epoch: 20 precision: 0.809796, recall: 0.808273, f1: 0.805804\n","191/191 [==============================] - 16s 82ms/step - loss: 0.1526 - acc: 0.9412 - val_loss: 0.5859 - val_acc: 0.8168\n","Epoch 22/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.1481 - acc: 0.9467Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.6740 - acc: 0.7905\n","epoch: 21 precision: 0.798442, recall: 0.799081, f1: 0.798486\n","191/191 [==============================] - 16s 81ms/step - loss: 0.1478 - acc: 0.9468 - val_loss: 0.6740 - val_acc: 0.7905\n","Epoch 23/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.1394 - acc: 0.9451Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.6758 - acc: 0.7965\n","epoch: 22 precision: 0.800031, recall: 0.799081, f1: 0.796639\n","191/191 [==============================] - 16s 81ms/step - loss: 0.1393 - acc: 0.9452 - val_loss: 0.6758 - val_acc: 0.7965\n","Epoch 24/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.1284 - acc: 0.9544Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.6713 - acc: 0.7971\n","epoch: 23 precision: 0.801813, recall: 0.801707, f1: 0.799903\n","191/191 [==============================] - 16s 82ms/step - loss: 0.1286 - acc: 0.9544 - val_loss: 0.6713 - val_acc: 0.7971\n","Epoch 25/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.1283 - acc: 0.9564Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.6548 - acc: 0.8089\n","epoch: 24 precision: 0.801100, recall: 0.801051, f1: 0.799272\n","191/191 [==============================] - 16s 81ms/step - loss: 0.1289 - acc: 0.9562 - val_loss: 0.6548 - val_acc: 0.8089\n","Epoch 26/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.1283 - acc: 0.9541Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.6398 - acc: 0.7938\n","epoch: 25 precision: 0.791881, recall: 0.792515, f1: 0.791998\n","191/191 [==============================] - 16s 81ms/step - loss: 0.1286 - acc: 0.9540 - val_loss: 0.6398 - val_acc: 0.7938\n","Epoch 27/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.1239 - acc: 0.9503Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.6969 - acc: 0.8004\n","epoch: 26 precision: 0.799695, recall: 0.797111, f1: 0.793757\n","191/191 [==============================] - 16s 81ms/step - loss: 0.1260 - acc: 0.9499 - val_loss: 0.6969 - val_acc: 0.8004\n","Epoch 28/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.1230 - acc: 0.9535Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.6917 - acc: 0.8043\n","epoch: 27 precision: 0.803856, recall: 0.803020, f1: 0.800766\n","191/191 [==============================] - 16s 81ms/step - loss: 0.1227 - acc: 0.9535 - val_loss: 0.6917 - val_acc: 0.8043\n","Epoch 29/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.1187 - acc: 0.9579Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.6565 - acc: 0.8017\n","epoch: 28 precision: 0.785896, recall: 0.786605, f1: 0.785999\n","191/191 [==============================] - 16s 82ms/step - loss: 0.1186 - acc: 0.9580 - val_loss: 0.6565 - val_acc: 0.8017\n","Epoch 30/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.1127 - acc: 0.9590Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.7176 - acc: 0.7853\n","epoch: 29 precision: 0.783833, recall: 0.784636, f1: 0.783788\n","191/191 [==============================] - 16s 82ms/step - loss: 0.1122 - acc: 0.9591 - val_loss: 0.7176 - val_acc: 0.7853\n","Epoch 31/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.1050 - acc: 0.9597Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.7525 - acc: 0.7859\n","epoch: 30 precision: 0.789822, recall: 0.790545, f1: 0.789850\n","191/191 [==============================] - 16s 81ms/step - loss: 0.1046 - acc: 0.9598 - val_loss: 0.7525 - val_acc: 0.7859\n","Epoch 32/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.1118 - acc: 0.9561Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.7333 - acc: 0.7820\n","epoch: 31 precision: 0.786133, recall: 0.784636, f1: 0.785118\n","191/191 [==============================] - 16s 81ms/step - loss: 0.1114 - acc: 0.9562 - val_loss: 0.7333 - val_acc: 0.7820\n","Epoch 33/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.1115 - acc: 0.9587Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.6916 - acc: 0.7814\n","epoch: 32 precision: 0.782972, recall: 0.782666, f1: 0.782802\n","191/191 [==============================] - 16s 81ms/step - loss: 0.1121 - acc: 0.9585 - val_loss: 0.6916 - val_acc: 0.7814\n","Epoch 34/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.1039 - acc: 0.9613Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.7399 - acc: 0.7991\n","epoch: 33 precision: 0.795761, recall: 0.796454, f1: 0.795601\n","191/191 [==============================] - 16s 81ms/step - loss: 0.1034 - acc: 0.9614 - val_loss: 0.7399 - val_acc: 0.7991\n","Epoch 35/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0995 - acc: 0.9602Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.7598 - acc: 0.7873\n","epoch: 34 precision: 0.781486, recall: 0.782009, f1: 0.781660\n","191/191 [==============================] - 16s 81ms/step - loss: 0.0994 - acc: 0.9603 - val_loss: 0.7598 - val_acc: 0.7873\n","Epoch 36/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.1036 - acc: 0.9618Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.7475 - acc: 0.7965\n","epoch: 35 precision: 0.795147, recall: 0.795798, f1: 0.795218\n","191/191 [==============================] - 15s 81ms/step - loss: 0.1037 - acc: 0.9619 - val_loss: 0.7475 - val_acc: 0.7965\n","Epoch 37/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.1003 - acc: 0.9618Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.7611 - acc: 0.7938\n","epoch: 36 precision: 0.791771, recall: 0.791858, f1: 0.789931\n","191/191 [==============================] - 16s 81ms/step - loss: 0.1002 - acc: 0.9619 - val_loss: 0.7611 - val_acc: 0.7938\n","Epoch 38/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0954 - acc: 0.9635Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.8507 - acc: 0.7748\n","epoch: 37 precision: 0.790225, recall: 0.790545, f1: 0.788805\n","191/191 [==============================] - 16s 82ms/step - loss: 0.0953 - acc: 0.9635 - val_loss: 0.8507 - val_acc: 0.7748\n","Epoch 39/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0942 - acc: 0.9633Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.8410 - acc: 0.7951\n","epoch: 38 precision: 0.804905, recall: 0.799081, f1: 0.794492\n","191/191 [==============================] - 16s 81ms/step - loss: 0.0938 - acc: 0.9634 - val_loss: 0.8410 - val_acc: 0.7951\n","Epoch 40/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0939 - acc: 0.9638Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.7885 - acc: 0.7905\n","epoch: 39 precision: 0.787190, recall: 0.787919, f1: 0.786719\n","191/191 [==============================] - 16s 81ms/step - loss: 0.0936 - acc: 0.9639 - val_loss: 0.7885 - val_acc: 0.7905\n","Epoch 41/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0944 - acc: 0.9628Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.7876 - acc: 0.7958\n","epoch: 40 precision: 0.795169, recall: 0.795798, f1: 0.795266\n","191/191 [==============================] - 16s 81ms/step - loss: 0.0945 - acc: 0.9627 - val_loss: 0.7876 - val_acc: 0.7958\n","Epoch 42/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0963 - acc: 0.9622Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.7990 - acc: 0.7741\n","epoch: 41 precision: 0.777264, recall: 0.778070, f1: 0.777360\n","191/191 [==============================] - 16s 81ms/step - loss: 0.0967 - acc: 0.9621 - val_loss: 0.7990 - val_acc: 0.7741\n","Epoch 43/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0964 - acc: 0.9660Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.8228 - acc: 0.7886\n","epoch: 42 precision: 0.783099, recall: 0.783322, f1: 0.783198\n","191/191 [==============================] - 16s 81ms/step - loss: 0.0960 - acc: 0.9660 - val_loss: 0.8228 - val_acc: 0.7886\n","Epoch 44/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0941 - acc: 0.9640Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.7748 - acc: 0.7971\n","epoch: 43 precision: 0.802215, recall: 0.800394, f1: 0.797525\n","191/191 [==============================] - 16s 81ms/step - loss: 0.0938 - acc: 0.9640 - val_loss: 0.7748 - val_acc: 0.7971\n","Epoch 45/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0910 - acc: 0.9646Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.7928 - acc: 0.7984\n","epoch: 44 precision: 0.795524, recall: 0.795798, f1: 0.794165\n","191/191 [==============================] - 15s 81ms/step - loss: 0.0921 - acc: 0.9645 - val_loss: 0.7928 - val_acc: 0.7984\n","Epoch 46/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0819 - acc: 0.9706Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.7798 - acc: 0.7879\n","epoch: 45 precision: 0.790557, recall: 0.791202, f1: 0.789933\n","191/191 [==============================] - 16s 81ms/step - loss: 0.0816 - acc: 0.9706 - val_loss: 0.7798 - val_acc: 0.7879\n","Epoch 47/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0883 - acc: 0.9673Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.8140 - acc: 0.8043\n","epoch: 46 precision: 0.804099, recall: 0.803020, f1: 0.800626\n","191/191 [==============================] - 16s 81ms/step - loss: 0.0881 - acc: 0.9673 - val_loss: 0.8140 - val_acc: 0.8043\n","Epoch 48/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0871 - acc: 0.9658Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.8226 - acc: 0.7866\n","epoch: 47 precision: 0.789608, recall: 0.789888, f1: 0.789726\n","191/191 [==============================] - 16s 81ms/step - loss: 0.0870 - acc: 0.9658 - val_loss: 0.8226 - val_acc: 0.7866\n","Epoch 49/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0931 - acc: 0.9637Epoch 1/100\n"," 47/191 [======>.......................] - ETA: 7s - loss: 0.8202 - acc: 0.8078\n","epoch: 48 precision: 0.809047, recall: 0.807617, f1: 0.805174\n","191/191 [==============================] - 15s 81ms/step - loss: 0.0926 - acc: 0.9637 - val_loss: 0.8249 - val_acc: 0.8076\n","Epoch 50/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0902 - acc: 0.9646Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.8490 - acc: 0.8083\n","epoch: 49 precision: 0.810134, recall: 0.808930, f1: 0.806642\n","191/191 [==============================] - 16s 81ms/step - loss: 0.0916 - acc: 0.9645 - val_loss: 0.8490 - val_acc: 0.8083\n","Epoch 51/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0863 - acc: 0.9678Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.8455 - acc: 0.8056\n","epoch: 50 precision: 0.791832, recall: 0.792515, f1: 0.791901\n","191/191 [==============================] - 16s 81ms/step - loss: 0.0890 - acc: 0.9675 - val_loss: 0.8455 - val_acc: 0.8056\n","Epoch 52/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0813 - acc: 0.9692Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.7995 - acc: 0.8024\n","epoch: 51 precision: 0.797190, recall: 0.797768, f1: 0.797310\n","191/191 [==============================] - 16s 81ms/step - loss: 0.0830 - acc: 0.9691 - val_loss: 0.7995 - val_acc: 0.8024\n","Epoch 53/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0907 - acc: 0.9646Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.8275 - acc: 0.7978\n","epoch: 52 precision: 0.797947, recall: 0.797768, f1: 0.795796\n","191/191 [==============================] - 16s 81ms/step - loss: 0.0909 - acc: 0.9645 - val_loss: 0.8275 - val_acc: 0.7978\n","Epoch 54/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0825 - acc: 0.9689Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.8939 - acc: 0.7859\n","epoch: 53 precision: 0.788680, recall: 0.789232, f1: 0.787737\n","191/191 [==============================] - 16s 81ms/step - loss: 0.0822 - acc: 0.9690 - val_loss: 0.8939 - val_acc: 0.7859\n","Epoch 55/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0832 - acc: 0.9674Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.8146 - acc: 0.7951\n","epoch: 54 precision: 0.785511, recall: 0.784636, f1: 0.784963\n","191/191 [==============================] - 16s 81ms/step - loss: 0.0849 - acc: 0.9672 - val_loss: 0.8146 - val_acc: 0.7951\n","Epoch 56/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0727 - acc: 0.9742Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.9000 - acc: 0.8004\n","epoch: 55 precision: 0.805305, recall: 0.804990, f1: 0.803121\n","191/191 [==============================] - 15s 81ms/step - loss: 0.0726 - acc: 0.9742 - val_loss: 0.9000 - val_acc: 0.8004\n","Epoch 57/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0786 - acc: 0.9699Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.9213 - acc: 0.7892\n","epoch: 56 precision: 0.797764, recall: 0.793171, f1: 0.788780\n","191/191 [==============================] - 16s 81ms/step - loss: 0.0783 - acc: 0.9700 - val_loss: 0.9213 - val_acc: 0.7892\n","Epoch 58/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0789 - acc: 0.9691Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.8607 - acc: 0.7879\n","epoch: 57 precision: 0.785183, recall: 0.785949, f1: 0.785212\n","191/191 [==============================] - 16s 83ms/step - loss: 0.0786 - acc: 0.9691 - val_loss: 0.8607 - val_acc: 0.7879\n","Epoch 59/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0794 - acc: 0.9688Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.8613 - acc: 0.7951\n","epoch: 58 precision: 0.799804, recall: 0.800394, f1: 0.799897\n","191/191 [==============================] - 16s 81ms/step - loss: 0.0792 - acc: 0.9688 - val_loss: 0.8613 - val_acc: 0.7951\n","Epoch 60/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0805 - acc: 0.9684Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.9429 - acc: 0.7971\n","epoch: 59 precision: 0.801765, recall: 0.799737, f1: 0.796744\n","191/191 [==============================] - 16s 81ms/step - loss: 0.0805 - acc: 0.9683 - val_loss: 0.9429 - val_acc: 0.7971\n","Epoch 61/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0805 - acc: 0.9678Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.8583 - acc: 0.8063\n","epoch: 60 precision: 0.807682, recall: 0.808273, f1: 0.807566\n","191/191 [==============================] - 15s 81ms/step - loss: 0.0802 - acc: 0.9678 - val_loss: 0.8583 - val_acc: 0.8063\n","Epoch 62/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0822 - acc: 0.9679Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.8313 - acc: 0.7978\n","epoch: 61 precision: 0.796490, recall: 0.797111, f1: 0.796582\n","191/191 [==============================] - 16s 81ms/step - loss: 0.0831 - acc: 0.9678 - val_loss: 0.8313 - val_acc: 0.7978\n","Epoch 63/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0787 - acc: 0.9683Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.8514 - acc: 0.7932\n","epoch: 62 precision: 0.800656, recall: 0.801051, f1: 0.799698\n","191/191 [==============================] - 15s 81ms/step - loss: 0.0807 - acc: 0.9678 - val_loss: 0.8514 - val_acc: 0.7932\n","Epoch 64/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0854 - acc: 0.9660Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.8569 - acc: 0.7807\n","epoch: 63 precision: 0.782626, recall: 0.782666, f1: 0.782646\n","191/191 [==============================] - 16s 81ms/step - loss: 0.0850 - acc: 0.9660 - val_loss: 0.8569 - val_acc: 0.7807\n","Epoch 65/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0836 - acc: 0.9669Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.8426 - acc: 0.7951\n","epoch: 64 precision: 0.791133, recall: 0.791858, f1: 0.791117\n","191/191 [==============================] - 16s 81ms/step - loss: 0.0832 - acc: 0.9670 - val_loss: 0.8426 - val_acc: 0.7951\n","Epoch 66/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0799 - acc: 0.9725Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.9043 - acc: 0.8017\n","epoch: 65 precision: 0.808123, recall: 0.806303, f1: 0.803630\n","191/191 [==============================] - 15s 81ms/step - loss: 0.0795 - acc: 0.9726 - val_loss: 0.9043 - val_acc: 0.8017\n","Epoch 67/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0815 - acc: 0.9702Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.8577 - acc: 0.7879\n","epoch: 66 precision: 0.787734, recall: 0.787919, f1: 0.787818\n","191/191 [==============================] - 15s 81ms/step - loss: 0.0811 - acc: 0.9703 - val_loss: 0.8577 - val_acc: 0.7879\n","Epoch 68/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0807 - acc: 0.9681Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.9181 - acc: 0.7938\n","epoch: 67 precision: 0.800653, recall: 0.797768, f1: 0.794301\n","191/191 [==============================] - 15s 81ms/step - loss: 0.0810 - acc: 0.9680 - val_loss: 0.9181 - val_acc: 0.7938\n","Epoch 69/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0771 - acc: 0.9674Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.8413 - acc: 0.8096\n","epoch: 68 precision: 0.803242, recall: 0.803020, f1: 0.801164\n","191/191 [==============================] - 15s 81ms/step - loss: 0.0768 - acc: 0.9675 - val_loss: 0.8413 - val_acc: 0.8096\n","Epoch 70/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0755 - acc: 0.9706Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.8483 - acc: 0.8056\n","epoch: 69 precision: 0.804529, recall: 0.804990, f1: 0.803833\n","191/191 [==============================] - 15s 81ms/step - loss: 0.0752 - acc: 0.9706 - val_loss: 0.8483 - val_acc: 0.8056\n","Epoch 71/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0774 - acc: 0.9689Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.8842 - acc: 0.7781\n","epoch: 70 precision: 0.780176, recall: 0.776100, f1: 0.776974\n","191/191 [==============================] - 15s 80ms/step - loss: 0.0771 - acc: 0.9690 - val_loss: 0.8842 - val_acc: 0.7781\n","Epoch 72/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0741 - acc: 0.9717Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.8282 - acc: 0.8063\n","epoch: 71 precision: 0.798577, recall: 0.798424, f1: 0.796492\n","191/191 [==============================] - 16s 82ms/step - loss: 0.0737 - acc: 0.9718 - val_loss: 0.8282 - val_acc: 0.8063\n","Epoch 73/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0768 - acc: 0.9688Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.9078 - acc: 0.7833\n","epoch: 72 precision: 0.783984, recall: 0.784636, f1: 0.783140\n","191/191 [==============================] - 16s 82ms/step - loss: 0.0791 - acc: 0.9686 - val_loss: 0.9078 - val_acc: 0.7833\n","Epoch 74/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0752 - acc: 0.9681Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.8914 - acc: 0.7866\n","epoch: 73 precision: 0.785906, recall: 0.786605, f1: 0.785278\n","191/191 [==============================] - 16s 82ms/step - loss: 0.0759 - acc: 0.9680 - val_loss: 0.8914 - val_acc: 0.7866\n","Epoch 75/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0787 - acc: 0.9697Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.8682 - acc: 0.7991\n","epoch: 74 precision: 0.798557, recall: 0.797768, f1: 0.795382\n","191/191 [==============================] - 16s 82ms/step - loss: 0.0785 - acc: 0.9698 - val_loss: 0.8682 - val_acc: 0.7991\n","Epoch 76/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0682 - acc: 0.9730Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.8656 - acc: 0.7971\n","epoch: 75 precision: 0.800819, recall: 0.801051, f1: 0.799520\n","191/191 [==============================] - 16s 81ms/step - loss: 0.0679 - acc: 0.9731 - val_loss: 0.8656 - val_acc: 0.7971\n","Epoch 77/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0779 - acc: 0.9701Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.8851 - acc: 0.7919\n","epoch: 76 precision: 0.802257, recall: 0.801051, f1: 0.798525\n","191/191 [==============================] - 16s 81ms/step - loss: 0.0777 - acc: 0.9701 - val_loss: 0.8851 - val_acc: 0.7919\n","Epoch 78/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0717 - acc: 0.9719Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.8411 - acc: 0.7958\n","epoch: 77 precision: 0.797296, recall: 0.797768, f1: 0.796423\n","191/191 [==============================] - 16s 82ms/step - loss: 0.0715 - acc: 0.9719 - val_loss: 0.8411 - val_acc: 0.7958\n","Epoch 79/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0654 - acc: 0.9725Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.8674 - acc: 0.8056\n","epoch: 78 precision: 0.804587, recall: 0.804334, f1: 0.802490\n","191/191 [==============================] - 15s 81ms/step - loss: 0.0654 - acc: 0.9726 - val_loss: 0.8674 - val_acc: 0.8056\n","Epoch 80/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0741 - acc: 0.9707Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.8253 - acc: 0.8056\n","epoch: 79 precision: 0.803787, recall: 0.803677, f1: 0.801922\n","191/191 [==============================] - 15s 81ms/step - loss: 0.0738 - acc: 0.9708 - val_loss: 0.8253 - val_acc: 0.8056\n","Epoch 81/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0694 - acc: 0.9725Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.9013 - acc: 0.8037\n","epoch: 80 precision: 0.804361, recall: 0.803020, f1: 0.800484\n","191/191 [==============================] - 16s 81ms/step - loss: 0.0691 - acc: 0.9726 - val_loss: 0.9013 - val_acc: 0.8037\n","Epoch 82/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0678 - acc: 0.9745Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.8682 - acc: 0.7905\n","epoch: 81 precision: 0.794448, recall: 0.795141, f1: 0.794176\n","191/191 [==============================] - 15s 81ms/step - loss: 0.0676 - acc: 0.9745 - val_loss: 0.8682 - val_acc: 0.7905\n","Epoch 83/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0715 - acc: 0.9722Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.8337 - acc: 0.7912\n","epoch: 82 precision: 0.790464, recall: 0.791202, f1: 0.790218\n","191/191 [==============================] - 15s 81ms/step - loss: 0.0712 - acc: 0.9722 - val_loss: 0.8337 - val_acc: 0.7912\n","Epoch 84/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0694 - acc: 0.9729Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.9457 - acc: 0.7846\n","epoch: 83 precision: 0.783322, recall: 0.783322, f1: 0.783322\n","191/191 [==============================] - 15s 81ms/step - loss: 0.0707 - acc: 0.9727 - val_loss: 0.9457 - val_acc: 0.7846\n","Epoch 85/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0718 - acc: 0.9714Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.9109 - acc: 0.8063\n","epoch: 84 precision: 0.805251, recall: 0.803677, f1: 0.801040\n","191/191 [==============================] - 15s 81ms/step - loss: 0.0719 - acc: 0.9713 - val_loss: 0.9109 - val_acc: 0.8063\n","Epoch 86/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0715 - acc: 0.9737Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.9386 - acc: 0.7978\n","epoch: 85 precision: 0.807281, recall: 0.807617, f1: 0.806365\n","191/191 [==============================] - 15s 81ms/step - loss: 0.0727 - acc: 0.9736 - val_loss: 0.9386 - val_acc: 0.7978\n","Epoch 87/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0683 - acc: 0.9738Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.8832 - acc: 0.7879\n","epoch: 86 precision: 0.785818, recall: 0.786605, f1: 0.785738\n","191/191 [==============================] - 15s 81ms/step - loss: 0.0684 - acc: 0.9737 - val_loss: 0.8832 - val_acc: 0.7879\n","Epoch 88/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0700 - acc: 0.9722Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.9252 - acc: 0.7938\n","epoch: 87 precision: 0.793032, recall: 0.792515, f1: 0.790140\n","191/191 [==============================] - 15s 81ms/step - loss: 0.0699 - acc: 0.9722 - val_loss: 0.9252 - val_acc: 0.7938\n","Epoch 89/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0680 - acc: 0.9740Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.8646 - acc: 0.7899\n","epoch: 88 precision: 0.791838, recall: 0.792515, f1: 0.791370\n","191/191 [==============================] - 15s 80ms/step - loss: 0.0696 - acc: 0.9737 - val_loss: 0.8646 - val_acc: 0.7899\n","Epoch 90/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0697 - acc: 0.9729Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.8175 - acc: 0.8109\n","epoch: 89 precision: 0.791861, recall: 0.792515, f1: 0.791312\n","191/191 [==============================] - 15s 81ms/step - loss: 0.0693 - acc: 0.9729 - val_loss: 0.8175 - val_acc: 0.8109\n","Epoch 91/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0756 - acc: 0.9709Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.8448 - acc: 0.7892\n","epoch: 90 precision: 0.786224, recall: 0.785292, f1: 0.785635\n","191/191 [==============================] - 15s 81ms/step - loss: 0.0762 - acc: 0.9708 - val_loss: 0.8448 - val_acc: 0.7892\n","Epoch 92/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0695 - acc: 0.9719Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.8580 - acc: 0.7892\n","epoch: 91 precision: 0.786566, recall: 0.786605, f1: 0.786585\n","191/191 [==============================] - 15s 81ms/step - loss: 0.0692 - acc: 0.9719 - val_loss: 0.8580 - val_acc: 0.7892\n","Epoch 93/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0660 - acc: 0.9745Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.8914 - acc: 0.7978\n","epoch: 92 precision: 0.799775, recall: 0.800394, f1: 0.799401\n","191/191 [==============================] - 15s 80ms/step - loss: 0.0660 - acc: 0.9745 - val_loss: 0.8914 - val_acc: 0.7978\n","Epoch 94/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0747 - acc: 0.9706Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.8024 - acc: 0.8024\n","epoch: 93 precision: 0.802223, recall: 0.802364, f1: 0.800783\n","191/191 [==============================] - 15s 81ms/step - loss: 0.0743 - acc: 0.9706 - val_loss: 0.8024 - val_acc: 0.8024\n","Epoch 95/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0677 - acc: 0.9729Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.9255 - acc: 0.8089\n","epoch: 94 precision: 0.807828, recall: 0.806303, f1: 0.803774\n","191/191 [==============================] - 15s 80ms/step - loss: 0.0674 - acc: 0.9729 - val_loss: 0.9255 - val_acc: 0.8089\n","Epoch 96/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0705 - acc: 0.9732Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.9608 - acc: 0.7846\n","epoch: 95 precision: 0.790533, recall: 0.791202, f1: 0.790633\n","191/191 [==============================] - 15s 81ms/step - loss: 0.0702 - acc: 0.9732 - val_loss: 0.9608 - val_acc: 0.7846\n","Epoch 97/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0742 - acc: 0.9711Epoch 1/100\n"," 47/191 [======>.......................] - ETA: 7s - loss: 0.9284 - acc: 0.8045\n","epoch: 96 precision: 0.804973, recall: 0.803677, f1: 0.801185\n","191/191 [==============================] - 15s 81ms/step - loss: 0.0743 - acc: 0.9709 - val_loss: 0.9276 - val_acc: 0.8037\n","Epoch 98/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0693 - acc: 0.9730Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.9389 - acc: 0.7853\n","epoch: 97 precision: 0.791814, recall: 0.792515, f1: 0.791851\n","191/191 [==============================] - 16s 81ms/step - loss: 0.0691 - acc: 0.9731 - val_loss: 0.9389 - val_acc: 0.7853\n","Epoch 99/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0650 - acc: 0.9734Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.9688 - acc: 0.8004\n","epoch: 98 precision: 0.795971, recall: 0.795798, f1: 0.793773\n","191/191 [==============================] - 15s 81ms/step - loss: 0.0652 - acc: 0.9732 - val_loss: 0.9688 - val_acc: 0.8004\n","Epoch 100/100\n","190/191 [============================>.] - ETA: 0s - loss: 0.0677 - acc: 0.9727Epoch 1/100\n"," 48/191 [======>.......................] - ETA: 7s - loss: 0.9432 - acc: 0.7794\n","epoch: 99 precision: 0.789321, recall: 0.789232, f1: 0.787072\n","191/191 [==============================] - 15s 81ms/step - loss: 0.0674 - acc: 0.9727 - val_loss: 0.9432 - val_acc: 0.7794\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f95aeb62630>"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"roAMVyKhTA9C","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":166},"executionInfo":{"status":"ok","timestamp":1596593801747,"user_tz":-480,"elapsed":2425,"user":{"displayName":"dy c","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiwUhiP2JGBJF0ccdzeCMO3DywIn5WL-okr917K=s64","userId":"07027550308680493425"}},"outputId":"bb36c37a-ce34-4834-deb6-c3fa38c23e2e"},"source":["model.evaluate(test_data_x,test_data_y,debug_info=True)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0     0.7886    0.8604    0.8229       867\n","           1     0.7903    0.6951    0.7397       656\n","\n","    accuracy                         0.7892      1523\n","   macro avg     0.7894    0.7778    0.7813      1523\n","weighted avg     0.7893    0.7892    0.7871      1523\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sitXX0mISfcf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1595926994660,"user_tz":-480,"elapsed":4532181,"user":{"displayName":"dy c","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiwUhiP2JGBJF0ccdzeCMO3DywIn5WL-okr917K=s64","userId":"07027550308680493425"}},"outputId":"e963c341-e8b4-4a98-a5d3-353fa71a4a4b"},"source":["from tensorflow.python import keras\n","from kashgari.tasks.classification import DPCNN_Model\n","import tensorflow as tf\n","from kashgari.callbacks import EvalCallBack\n","#hyper = DPCNN_Model.get_default_hyper_parameters()\n","#hyper['rnn_0']['units'] = 250\n","#hyper['rnn_1']['units'] = 250\n","model = DPCNN_Model(bert_embed)#,hyper_parameters=hyper)\n","# 这是 Kashgari 内置回调函数，会在训练过程计算精确度，召回率和 F1\n","eval_callback = EvalCallBack(kash_model=model,\n","                             valid_x=test_data_x,\n","                             valid_y=test_data_y,\n","                             step=1)\n","early_stop = keras.callbacks.EarlyStopping(patience=100)\n","model.fit(training_data_x,\n","          training_data_y,\n","          test_data_x,\n","          test_data_y,\n","          batch_size=16,epochs=20,callbacks=[eval_callback,early_stop])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"model_10\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","Input-Token (InputLayer)        [(None, 100)]        0                                            \n","__________________________________________________________________________________________________\n","Input-Segment (InputLayer)      [(None, 100)]        0                                            \n","__________________________________________________________________________________________________\n","Embedding-Token (TokenEmbedding [(None, 100, 1024),  29691904    Input-Token[0][0]                \n","__________________________________________________________________________________________________\n","Embedding-Segment (Embedding)   (None, 100, 1024)    2048        Input-Segment[0][0]              \n","__________________________________________________________________________________________________\n","Embedding-Token-Segment (Add)   (None, 100, 1024)    0           Embedding-Token[0][0]            \n","                                                                 Embedding-Segment[0][0]          \n","__________________________________________________________________________________________________\n","Embedding-Position (PositionEmb (None, 100, 1024)    102400      Embedding-Token-Segment[0][0]    \n","__________________________________________________________________________________________________\n","Embedding-Dropout (Dropout)     (None, 100, 1024)    0           Embedding-Position[0][0]         \n","__________________________________________________________________________________________________\n","Embedding-Norm (LayerNormalizat (None, 100, 1024)    2048        Embedding-Dropout[0][0]          \n","__________________________________________________________________________________________________\n","Encoder-1-MultiHeadSelfAttentio (None, 100, 1024)    4198400     Embedding-Norm[0][0]             \n","__________________________________________________________________________________________________\n","Encoder-1-MultiHeadSelfAttentio (None, 100, 1024)    0           Encoder-1-MultiHeadSelfAttention[\n","__________________________________________________________________________________________________\n","Encoder-1-MultiHeadSelfAttentio (None, 100, 1024)    0           Embedding-Norm[0][0]             \n","                                                                 Encoder-1-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-1-MultiHeadSelfAttentio (None, 100, 1024)    2048        Encoder-1-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-1-FeedForward (FeedForw (None, 100, 1024)    8393728     Encoder-1-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-1-FeedForward-Dropout ( (None, 100, 1024)    0           Encoder-1-FeedForward[0][0]      \n","__________________________________________________________________________________________________\n","Encoder-1-FeedForward-Add (Add) (None, 100, 1024)    0           Encoder-1-MultiHeadSelfAttention-\n","                                                                 Encoder-1-FeedForward-Dropout[0][\n","__________________________________________________________________________________________________\n","Encoder-1-FeedForward-Norm (Lay (None, 100, 1024)    2048        Encoder-1-FeedForward-Add[0][0]  \n","__________________________________________________________________________________________________\n","Encoder-2-MultiHeadSelfAttentio (None, 100, 1024)    4198400     Encoder-1-FeedForward-Norm[0][0] \n","__________________________________________________________________________________________________\n","Encoder-2-MultiHeadSelfAttentio (None, 100, 1024)    0           Encoder-2-MultiHeadSelfAttention[\n","__________________________________________________________________________________________________\n","Encoder-2-MultiHeadSelfAttentio (None, 100, 1024)    0           Encoder-1-FeedForward-Norm[0][0] \n","                                                                 Encoder-2-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-2-MultiHeadSelfAttentio (None, 100, 1024)    2048        Encoder-2-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-2-FeedForward (FeedForw (None, 100, 1024)    8393728     Encoder-2-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-2-FeedForward-Dropout ( (None, 100, 1024)    0           Encoder-2-FeedForward[0][0]      \n","__________________________________________________________________________________________________\n","Encoder-2-FeedForward-Add (Add) (None, 100, 1024)    0           Encoder-2-MultiHeadSelfAttention-\n","                                                                 Encoder-2-FeedForward-Dropout[0][\n","__________________________________________________________________________________________________\n","Encoder-2-FeedForward-Norm (Lay (None, 100, 1024)    2048        Encoder-2-FeedForward-Add[0][0]  \n","__________________________________________________________________________________________________\n","Encoder-3-MultiHeadSelfAttentio (None, 100, 1024)    4198400     Encoder-2-FeedForward-Norm[0][0] \n","__________________________________________________________________________________________________\n","Encoder-3-MultiHeadSelfAttentio (None, 100, 1024)    0           Encoder-3-MultiHeadSelfAttention[\n","__________________________________________________________________________________________________\n","Encoder-3-MultiHeadSelfAttentio (None, 100, 1024)    0           Encoder-2-FeedForward-Norm[0][0] \n","                                                                 Encoder-3-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-3-MultiHeadSelfAttentio (None, 100, 1024)    2048        Encoder-3-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-3-FeedForward (FeedForw (None, 100, 1024)    8393728     Encoder-3-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-3-FeedForward-Dropout ( (None, 100, 1024)    0           Encoder-3-FeedForward[0][0]      \n","__________________________________________________________________________________________________\n","Encoder-3-FeedForward-Add (Add) (None, 100, 1024)    0           Encoder-3-MultiHeadSelfAttention-\n","                                                                 Encoder-3-FeedForward-Dropout[0][\n","__________________________________________________________________________________________________\n","Encoder-3-FeedForward-Norm (Lay (None, 100, 1024)    2048        Encoder-3-FeedForward-Add[0][0]  \n","__________________________________________________________________________________________________\n","Encoder-4-MultiHeadSelfAttentio (None, 100, 1024)    4198400     Encoder-3-FeedForward-Norm[0][0] \n","__________________________________________________________________________________________________\n","Encoder-4-MultiHeadSelfAttentio (None, 100, 1024)    0           Encoder-4-MultiHeadSelfAttention[\n","__________________________________________________________________________________________________\n","Encoder-4-MultiHeadSelfAttentio (None, 100, 1024)    0           Encoder-3-FeedForward-Norm[0][0] \n","                                                                 Encoder-4-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-4-MultiHeadSelfAttentio (None, 100, 1024)    2048        Encoder-4-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-4-FeedForward (FeedForw (None, 100, 1024)    8393728     Encoder-4-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-4-FeedForward-Dropout ( (None, 100, 1024)    0           Encoder-4-FeedForward[0][0]      \n","__________________________________________________________________________________________________\n","Encoder-4-FeedForward-Add (Add) (None, 100, 1024)    0           Encoder-4-MultiHeadSelfAttention-\n","                                                                 Encoder-4-FeedForward-Dropout[0][\n","__________________________________________________________________________________________________\n","Encoder-4-FeedForward-Norm (Lay (None, 100, 1024)    2048        Encoder-4-FeedForward-Add[0][0]  \n","__________________________________________________________________________________________________\n","Encoder-5-MultiHeadSelfAttentio (None, 100, 1024)    4198400     Encoder-4-FeedForward-Norm[0][0] \n","__________________________________________________________________________________________________\n","Encoder-5-MultiHeadSelfAttentio (None, 100, 1024)    0           Encoder-5-MultiHeadSelfAttention[\n","__________________________________________________________________________________________________\n","Encoder-5-MultiHeadSelfAttentio (None, 100, 1024)    0           Encoder-4-FeedForward-Norm[0][0] \n","                                                                 Encoder-5-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-5-MultiHeadSelfAttentio (None, 100, 1024)    2048        Encoder-5-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-5-FeedForward (FeedForw (None, 100, 1024)    8393728     Encoder-5-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-5-FeedForward-Dropout ( (None, 100, 1024)    0           Encoder-5-FeedForward[0][0]      \n","__________________________________________________________________________________________________\n","Encoder-5-FeedForward-Add (Add) (None, 100, 1024)    0           Encoder-5-MultiHeadSelfAttention-\n","                                                                 Encoder-5-FeedForward-Dropout[0][\n","__________________________________________________________________________________________________\n","Encoder-5-FeedForward-Norm (Lay (None, 100, 1024)    2048        Encoder-5-FeedForward-Add[0][0]  \n","__________________________________________________________________________________________________\n","Encoder-6-MultiHeadSelfAttentio (None, 100, 1024)    4198400     Encoder-5-FeedForward-Norm[0][0] \n","__________________________________________________________________________________________________\n","Encoder-6-MultiHeadSelfAttentio (None, 100, 1024)    0           Encoder-6-MultiHeadSelfAttention[\n","__________________________________________________________________________________________________\n","Encoder-6-MultiHeadSelfAttentio (None, 100, 1024)    0           Encoder-5-FeedForward-Norm[0][0] \n","                                                                 Encoder-6-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-6-MultiHeadSelfAttentio (None, 100, 1024)    2048        Encoder-6-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-6-FeedForward (FeedForw (None, 100, 1024)    8393728     Encoder-6-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-6-FeedForward-Dropout ( (None, 100, 1024)    0           Encoder-6-FeedForward[0][0]      \n","__________________________________________________________________________________________________\n","Encoder-6-FeedForward-Add (Add) (None, 100, 1024)    0           Encoder-6-MultiHeadSelfAttention-\n","                                                                 Encoder-6-FeedForward-Dropout[0][\n","__________________________________________________________________________________________________\n","Encoder-6-FeedForward-Norm (Lay (None, 100, 1024)    2048        Encoder-6-FeedForward-Add[0][0]  \n","__________________________________________________________________________________________________\n","Encoder-7-MultiHeadSelfAttentio (None, 100, 1024)    4198400     Encoder-6-FeedForward-Norm[0][0] \n","__________________________________________________________________________________________________\n","Encoder-7-MultiHeadSelfAttentio (None, 100, 1024)    0           Encoder-7-MultiHeadSelfAttention[\n","__________________________________________________________________________________________________\n","Encoder-7-MultiHeadSelfAttentio (None, 100, 1024)    0           Encoder-6-FeedForward-Norm[0][0] \n","                                                                 Encoder-7-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-7-MultiHeadSelfAttentio (None, 100, 1024)    2048        Encoder-7-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-7-FeedForward (FeedForw (None, 100, 1024)    8393728     Encoder-7-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-7-FeedForward-Dropout ( (None, 100, 1024)    0           Encoder-7-FeedForward[0][0]      \n","__________________________________________________________________________________________________\n","Encoder-7-FeedForward-Add (Add) (None, 100, 1024)    0           Encoder-7-MultiHeadSelfAttention-\n","                                                                 Encoder-7-FeedForward-Dropout[0][\n","__________________________________________________________________________________________________\n","Encoder-7-FeedForward-Norm (Lay (None, 100, 1024)    2048        Encoder-7-FeedForward-Add[0][0]  \n","__________________________________________________________________________________________________\n","Encoder-8-MultiHeadSelfAttentio (None, 100, 1024)    4198400     Encoder-7-FeedForward-Norm[0][0] \n","__________________________________________________________________________________________________\n","Encoder-8-MultiHeadSelfAttentio (None, 100, 1024)    0           Encoder-8-MultiHeadSelfAttention[\n","__________________________________________________________________________________________________\n","Encoder-8-MultiHeadSelfAttentio (None, 100, 1024)    0           Encoder-7-FeedForward-Norm[0][0] \n","                                                                 Encoder-8-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-8-MultiHeadSelfAttentio (None, 100, 1024)    2048        Encoder-8-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-8-FeedForward (FeedForw (None, 100, 1024)    8393728     Encoder-8-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-8-FeedForward-Dropout ( (None, 100, 1024)    0           Encoder-8-FeedForward[0][0]      \n","__________________________________________________________________________________________________\n","Encoder-8-FeedForward-Add (Add) (None, 100, 1024)    0           Encoder-8-MultiHeadSelfAttention-\n","                                                                 Encoder-8-FeedForward-Dropout[0][\n","__________________________________________________________________________________________________\n","Encoder-8-FeedForward-Norm (Lay (None, 100, 1024)    2048        Encoder-8-FeedForward-Add[0][0]  \n","__________________________________________________________________________________________________\n","Encoder-9-MultiHeadSelfAttentio (None, 100, 1024)    4198400     Encoder-8-FeedForward-Norm[0][0] \n","__________________________________________________________________________________________________\n","Encoder-9-MultiHeadSelfAttentio (None, 100, 1024)    0           Encoder-9-MultiHeadSelfAttention[\n","__________________________________________________________________________________________________\n","Encoder-9-MultiHeadSelfAttentio (None, 100, 1024)    0           Encoder-8-FeedForward-Norm[0][0] \n","                                                                 Encoder-9-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-9-MultiHeadSelfAttentio (None, 100, 1024)    2048        Encoder-9-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-9-FeedForward (FeedForw (None, 100, 1024)    8393728     Encoder-9-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-9-FeedForward-Dropout ( (None, 100, 1024)    0           Encoder-9-FeedForward[0][0]      \n","__________________________________________________________________________________________________\n","Encoder-9-FeedForward-Add (Add) (None, 100, 1024)    0           Encoder-9-MultiHeadSelfAttention-\n","                                                                 Encoder-9-FeedForward-Dropout[0][\n","__________________________________________________________________________________________________\n","Encoder-9-FeedForward-Norm (Lay (None, 100, 1024)    2048        Encoder-9-FeedForward-Add[0][0]  \n","__________________________________________________________________________________________________\n","Encoder-10-MultiHeadSelfAttenti (None, 100, 1024)    4198400     Encoder-9-FeedForward-Norm[0][0] \n","__________________________________________________________________________________________________\n","Encoder-10-MultiHeadSelfAttenti (None, 100, 1024)    0           Encoder-10-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-10-MultiHeadSelfAttenti (None, 100, 1024)    0           Encoder-9-FeedForward-Norm[0][0] \n","                                                                 Encoder-10-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-10-MultiHeadSelfAttenti (None, 100, 1024)    2048        Encoder-10-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-10-FeedForward (FeedFor (None, 100, 1024)    8393728     Encoder-10-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-10-FeedForward-Dropout  (None, 100, 1024)    0           Encoder-10-FeedForward[0][0]     \n","__________________________________________________________________________________________________\n","Encoder-10-FeedForward-Add (Add (None, 100, 1024)    0           Encoder-10-MultiHeadSelfAttention\n","                                                                 Encoder-10-FeedForward-Dropout[0]\n","__________________________________________________________________________________________________\n","Encoder-10-FeedForward-Norm (La (None, 100, 1024)    2048        Encoder-10-FeedForward-Add[0][0] \n","__________________________________________________________________________________________________\n","Encoder-11-MultiHeadSelfAttenti (None, 100, 1024)    4198400     Encoder-10-FeedForward-Norm[0][0]\n","__________________________________________________________________________________________________\n","Encoder-11-MultiHeadSelfAttenti (None, 100, 1024)    0           Encoder-11-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-11-MultiHeadSelfAttenti (None, 100, 1024)    0           Encoder-10-FeedForward-Norm[0][0]\n","                                                                 Encoder-11-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-11-MultiHeadSelfAttenti (None, 100, 1024)    2048        Encoder-11-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-11-FeedForward (FeedFor (None, 100, 1024)    8393728     Encoder-11-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-11-FeedForward-Dropout  (None, 100, 1024)    0           Encoder-11-FeedForward[0][0]     \n","__________________________________________________________________________________________________\n","Encoder-11-FeedForward-Add (Add (None, 100, 1024)    0           Encoder-11-MultiHeadSelfAttention\n","                                                                 Encoder-11-FeedForward-Dropout[0]\n","__________________________________________________________________________________________________\n","Encoder-11-FeedForward-Norm (La (None, 100, 1024)    2048        Encoder-11-FeedForward-Add[0][0] \n","__________________________________________________________________________________________________\n","Encoder-12-MultiHeadSelfAttenti (None, 100, 1024)    4198400     Encoder-11-FeedForward-Norm[0][0]\n","__________________________________________________________________________________________________\n","Encoder-12-MultiHeadSelfAttenti (None, 100, 1024)    0           Encoder-12-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-12-MultiHeadSelfAttenti (None, 100, 1024)    0           Encoder-11-FeedForward-Norm[0][0]\n","                                                                 Encoder-12-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-12-MultiHeadSelfAttenti (None, 100, 1024)    2048        Encoder-12-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-12-FeedForward (FeedFor (None, 100, 1024)    8393728     Encoder-12-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-12-FeedForward-Dropout  (None, 100, 1024)    0           Encoder-12-FeedForward[0][0]     \n","__________________________________________________________________________________________________\n","Encoder-12-FeedForward-Add (Add (None, 100, 1024)    0           Encoder-12-MultiHeadSelfAttention\n","                                                                 Encoder-12-FeedForward-Dropout[0]\n","__________________________________________________________________________________________________\n","Encoder-12-FeedForward-Norm (La (None, 100, 1024)    2048        Encoder-12-FeedForward-Add[0][0] \n","__________________________________________________________________________________________________\n","Encoder-13-MultiHeadSelfAttenti (None, 100, 1024)    4198400     Encoder-12-FeedForward-Norm[0][0]\n","__________________________________________________________________________________________________\n","Encoder-13-MultiHeadSelfAttenti (None, 100, 1024)    0           Encoder-13-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-13-MultiHeadSelfAttenti (None, 100, 1024)    0           Encoder-12-FeedForward-Norm[0][0]\n","                                                                 Encoder-13-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-13-MultiHeadSelfAttenti (None, 100, 1024)    2048        Encoder-13-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-13-FeedForward (FeedFor (None, 100, 1024)    8393728     Encoder-13-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-13-FeedForward-Dropout  (None, 100, 1024)    0           Encoder-13-FeedForward[0][0]     \n","__________________________________________________________________________________________________\n","Encoder-13-FeedForward-Add (Add (None, 100, 1024)    0           Encoder-13-MultiHeadSelfAttention\n","                                                                 Encoder-13-FeedForward-Dropout[0]\n","__________________________________________________________________________________________________\n","Encoder-13-FeedForward-Norm (La (None, 100, 1024)    2048        Encoder-13-FeedForward-Add[0][0] \n","__________________________________________________________________________________________________\n","Encoder-14-MultiHeadSelfAttenti (None, 100, 1024)    4198400     Encoder-13-FeedForward-Norm[0][0]\n","__________________________________________________________________________________________________\n","Encoder-14-MultiHeadSelfAttenti (None, 100, 1024)    0           Encoder-14-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-14-MultiHeadSelfAttenti (None, 100, 1024)    0           Encoder-13-FeedForward-Norm[0][0]\n","                                                                 Encoder-14-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-14-MultiHeadSelfAttenti (None, 100, 1024)    2048        Encoder-14-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-14-FeedForward (FeedFor (None, 100, 1024)    8393728     Encoder-14-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-14-FeedForward-Dropout  (None, 100, 1024)    0           Encoder-14-FeedForward[0][0]     \n","__________________________________________________________________________________________________\n","Encoder-14-FeedForward-Add (Add (None, 100, 1024)    0           Encoder-14-MultiHeadSelfAttention\n","                                                                 Encoder-14-FeedForward-Dropout[0]\n","__________________________________________________________________________________________________\n","Encoder-14-FeedForward-Norm (La (None, 100, 1024)    2048        Encoder-14-FeedForward-Add[0][0] \n","__________________________________________________________________________________________________\n","Encoder-15-MultiHeadSelfAttenti (None, 100, 1024)    4198400     Encoder-14-FeedForward-Norm[0][0]\n","__________________________________________________________________________________________________\n","Encoder-15-MultiHeadSelfAttenti (None, 100, 1024)    0           Encoder-15-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-15-MultiHeadSelfAttenti (None, 100, 1024)    0           Encoder-14-FeedForward-Norm[0][0]\n","                                                                 Encoder-15-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-15-MultiHeadSelfAttenti (None, 100, 1024)    2048        Encoder-15-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-15-FeedForward (FeedFor (None, 100, 1024)    8393728     Encoder-15-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-15-FeedForward-Dropout  (None, 100, 1024)    0           Encoder-15-FeedForward[0][0]     \n","__________________________________________________________________________________________________\n","Encoder-15-FeedForward-Add (Add (None, 100, 1024)    0           Encoder-15-MultiHeadSelfAttention\n","                                                                 Encoder-15-FeedForward-Dropout[0]\n","__________________________________________________________________________________________________\n","Encoder-15-FeedForward-Norm (La (None, 100, 1024)    2048        Encoder-15-FeedForward-Add[0][0] \n","__________________________________________________________________________________________________\n","Encoder-16-MultiHeadSelfAttenti (None, 100, 1024)    4198400     Encoder-15-FeedForward-Norm[0][0]\n","__________________________________________________________________________________________________\n","Encoder-16-MultiHeadSelfAttenti (None, 100, 1024)    0           Encoder-16-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-16-MultiHeadSelfAttenti (None, 100, 1024)    0           Encoder-15-FeedForward-Norm[0][0]\n","                                                                 Encoder-16-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-16-MultiHeadSelfAttenti (None, 100, 1024)    2048        Encoder-16-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-16-FeedForward (FeedFor (None, 100, 1024)    8393728     Encoder-16-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-16-FeedForward-Dropout  (None, 100, 1024)    0           Encoder-16-FeedForward[0][0]     \n","__________________________________________________________________________________________________\n","Encoder-16-FeedForward-Add (Add (None, 100, 1024)    0           Encoder-16-MultiHeadSelfAttention\n","                                                                 Encoder-16-FeedForward-Dropout[0]\n","__________________________________________________________________________________________________\n","Encoder-16-FeedForward-Norm (La (None, 100, 1024)    2048        Encoder-16-FeedForward-Add[0][0] \n","__________________________________________________________________________________________________\n","Encoder-17-MultiHeadSelfAttenti (None, 100, 1024)    4198400     Encoder-16-FeedForward-Norm[0][0]\n","__________________________________________________________________________________________________\n","Encoder-17-MultiHeadSelfAttenti (None, 100, 1024)    0           Encoder-17-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-17-MultiHeadSelfAttenti (None, 100, 1024)    0           Encoder-16-FeedForward-Norm[0][0]\n","                                                                 Encoder-17-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-17-MultiHeadSelfAttenti (None, 100, 1024)    2048        Encoder-17-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-17-FeedForward (FeedFor (None, 100, 1024)    8393728     Encoder-17-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-17-FeedForward-Dropout  (None, 100, 1024)    0           Encoder-17-FeedForward[0][0]     \n","__________________________________________________________________________________________________\n","Encoder-17-FeedForward-Add (Add (None, 100, 1024)    0           Encoder-17-MultiHeadSelfAttention\n","                                                                 Encoder-17-FeedForward-Dropout[0]\n","__________________________________________________________________________________________________\n","Encoder-17-FeedForward-Norm (La (None, 100, 1024)    2048        Encoder-17-FeedForward-Add[0][0] \n","__________________________________________________________________________________________________\n","Encoder-18-MultiHeadSelfAttenti (None, 100, 1024)    4198400     Encoder-17-FeedForward-Norm[0][0]\n","__________________________________________________________________________________________________\n","Encoder-18-MultiHeadSelfAttenti (None, 100, 1024)    0           Encoder-18-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-18-MultiHeadSelfAttenti (None, 100, 1024)    0           Encoder-17-FeedForward-Norm[0][0]\n","                                                                 Encoder-18-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-18-MultiHeadSelfAttenti (None, 100, 1024)    2048        Encoder-18-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-18-FeedForward (FeedFor (None, 100, 1024)    8393728     Encoder-18-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-18-FeedForward-Dropout  (None, 100, 1024)    0           Encoder-18-FeedForward[0][0]     \n","__________________________________________________________________________________________________\n","Encoder-18-FeedForward-Add (Add (None, 100, 1024)    0           Encoder-18-MultiHeadSelfAttention\n","                                                                 Encoder-18-FeedForward-Dropout[0]\n","__________________________________________________________________________________________________\n","Encoder-18-FeedForward-Norm (La (None, 100, 1024)    2048        Encoder-18-FeedForward-Add[0][0] \n","__________________________________________________________________________________________________\n","Encoder-19-MultiHeadSelfAttenti (None, 100, 1024)    4198400     Encoder-18-FeedForward-Norm[0][0]\n","__________________________________________________________________________________________________\n","Encoder-19-MultiHeadSelfAttenti (None, 100, 1024)    0           Encoder-19-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-19-MultiHeadSelfAttenti (None, 100, 1024)    0           Encoder-18-FeedForward-Norm[0][0]\n","                                                                 Encoder-19-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-19-MultiHeadSelfAttenti (None, 100, 1024)    2048        Encoder-19-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-19-FeedForward (FeedFor (None, 100, 1024)    8393728     Encoder-19-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-19-FeedForward-Dropout  (None, 100, 1024)    0           Encoder-19-FeedForward[0][0]     \n","__________________________________________________________________________________________________\n","Encoder-19-FeedForward-Add (Add (None, 100, 1024)    0           Encoder-19-MultiHeadSelfAttention\n","                                                                 Encoder-19-FeedForward-Dropout[0]\n","__________________________________________________________________________________________________\n","Encoder-19-FeedForward-Norm (La (None, 100, 1024)    2048        Encoder-19-FeedForward-Add[0][0] \n","__________________________________________________________________________________________________\n","Encoder-20-MultiHeadSelfAttenti (None, 100, 1024)    4198400     Encoder-19-FeedForward-Norm[0][0]\n","__________________________________________________________________________________________________\n","Encoder-20-MultiHeadSelfAttenti (None, 100, 1024)    0           Encoder-20-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-20-MultiHeadSelfAttenti (None, 100, 1024)    0           Encoder-19-FeedForward-Norm[0][0]\n","                                                                 Encoder-20-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-20-MultiHeadSelfAttenti (None, 100, 1024)    2048        Encoder-20-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-20-FeedForward (FeedFor (None, 100, 1024)    8393728     Encoder-20-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-20-FeedForward-Dropout  (None, 100, 1024)    0           Encoder-20-FeedForward[0][0]     \n","__________________________________________________________________________________________________\n","Encoder-20-FeedForward-Add (Add (None, 100, 1024)    0           Encoder-20-MultiHeadSelfAttention\n","                                                                 Encoder-20-FeedForward-Dropout[0]\n","__________________________________________________________________________________________________\n","Encoder-20-FeedForward-Norm (La (None, 100, 1024)    2048        Encoder-20-FeedForward-Add[0][0] \n","__________________________________________________________________________________________________\n","Encoder-21-MultiHeadSelfAttenti (None, 100, 1024)    4198400     Encoder-20-FeedForward-Norm[0][0]\n","__________________________________________________________________________________________________\n","Encoder-21-MultiHeadSelfAttenti (None, 100, 1024)    0           Encoder-21-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-21-MultiHeadSelfAttenti (None, 100, 1024)    0           Encoder-20-FeedForward-Norm[0][0]\n","                                                                 Encoder-21-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-21-MultiHeadSelfAttenti (None, 100, 1024)    2048        Encoder-21-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-21-FeedForward (FeedFor (None, 100, 1024)    8393728     Encoder-21-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-21-FeedForward-Dropout  (None, 100, 1024)    0           Encoder-21-FeedForward[0][0]     \n","__________________________________________________________________________________________________\n","Encoder-21-FeedForward-Add (Add (None, 100, 1024)    0           Encoder-21-MultiHeadSelfAttention\n","                                                                 Encoder-21-FeedForward-Dropout[0]\n","__________________________________________________________________________________________________\n","Encoder-21-FeedForward-Norm (La (None, 100, 1024)    2048        Encoder-21-FeedForward-Add[0][0] \n","__________________________________________________________________________________________________\n","Encoder-22-MultiHeadSelfAttenti (None, 100, 1024)    4198400     Encoder-21-FeedForward-Norm[0][0]\n","__________________________________________________________________________________________________\n","Encoder-22-MultiHeadSelfAttenti (None, 100, 1024)    0           Encoder-22-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-22-MultiHeadSelfAttenti (None, 100, 1024)    0           Encoder-21-FeedForward-Norm[0][0]\n","                                                                 Encoder-22-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-22-MultiHeadSelfAttenti (None, 100, 1024)    2048        Encoder-22-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-22-FeedForward (FeedFor (None, 100, 1024)    8393728     Encoder-22-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-22-FeedForward-Dropout  (None, 100, 1024)    0           Encoder-22-FeedForward[0][0]     \n","__________________________________________________________________________________________________\n","Encoder-22-FeedForward-Add (Add (None, 100, 1024)    0           Encoder-22-MultiHeadSelfAttention\n","                                                                 Encoder-22-FeedForward-Dropout[0]\n","__________________________________________________________________________________________________\n","Encoder-22-FeedForward-Norm (La (None, 100, 1024)    2048        Encoder-22-FeedForward-Add[0][0] \n","__________________________________________________________________________________________________\n","Encoder-23-MultiHeadSelfAttenti (None, 100, 1024)    4198400     Encoder-22-FeedForward-Norm[0][0]\n","__________________________________________________________________________________________________\n","Encoder-23-MultiHeadSelfAttenti (None, 100, 1024)    0           Encoder-23-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-23-MultiHeadSelfAttenti (None, 100, 1024)    0           Encoder-22-FeedForward-Norm[0][0]\n","                                                                 Encoder-23-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-23-MultiHeadSelfAttenti (None, 100, 1024)    2048        Encoder-23-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-23-FeedForward (FeedFor (None, 100, 1024)    8393728     Encoder-23-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-23-FeedForward-Dropout  (None, 100, 1024)    0           Encoder-23-FeedForward[0][0]     \n","__________________________________________________________________________________________________\n","Encoder-23-FeedForward-Add (Add (None, 100, 1024)    0           Encoder-23-MultiHeadSelfAttention\n","                                                                 Encoder-23-FeedForward-Dropout[0]\n","__________________________________________________________________________________________________\n","Encoder-23-FeedForward-Norm (La (None, 100, 1024)    2048        Encoder-23-FeedForward-Add[0][0] \n","__________________________________________________________________________________________________\n","Encoder-24-MultiHeadSelfAttenti (None, 100, 1024)    4198400     Encoder-23-FeedForward-Norm[0][0]\n","__________________________________________________________________________________________________\n","Encoder-24-MultiHeadSelfAttenti (None, 100, 1024)    0           Encoder-24-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-24-MultiHeadSelfAttenti (None, 100, 1024)    0           Encoder-23-FeedForward-Norm[0][0]\n","                                                                 Encoder-24-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-24-MultiHeadSelfAttenti (None, 100, 1024)    2048        Encoder-24-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-24-FeedForward (FeedFor (None, 100, 1024)    8393728     Encoder-24-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-24-FeedForward-Dropout  (None, 100, 1024)    0           Encoder-24-FeedForward[0][0]     \n","__________________________________________________________________________________________________\n","Encoder-24-FeedForward-Add (Add (None, 100, 1024)    0           Encoder-24-MultiHeadSelfAttention\n","                                                                 Encoder-24-FeedForward-Dropout[0]\n","__________________________________________________________________________________________________\n","Encoder-24-FeedForward-Norm (La (None, 100, 1024)    2048        Encoder-24-FeedForward-Add[0][0] \n","__________________________________________________________________________________________________\n","Encoder-Output (Concatenate)    (None, 100, 4096)    0           Encoder-21-FeedForward-Norm[0][0]\n","                                                                 Encoder-22-FeedForward-Norm[0][0]\n","                                                                 Encoder-23-FeedForward-Norm[0][0]\n","                                                                 Encoder-24-FeedForward-Norm[0][0]\n","__________________________________________________________________________________________________\n","non_masking_layer_1 (NonMasking (None, 100, 4096)    0           Encoder-Output[0][0]             \n","__________________________________________________________________________________________________\n","region_embedding (Conv1D)       (None, 100, 250)     3072250     non_masking_layer_1[0][0]        \n","__________________________________________________________________________________________________\n","batch_normalization (BatchNorma (None, 100, 250)     1000        region_embedding[0][0]           \n","__________________________________________________________________________________________________\n","p_re_lu (PReLU)                 (None, 100, 250)     25000       batch_normalization[0][0]        \n","__________________________________________________________________________________________________\n","dropout (Dropout)               (None, 100, 250)     0           p_re_lu[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_2 (BatchNor (None, 100, 250)     1000        dropout[0][0]                    \n","                                                                 conv1d[0][0]                     \n","__________________________________________________________________________________________________\n","p_re_lu_2 (PReLU)               (None, 100, 250)     25000       batch_normalization_2[0][0]      \n","                                                                 batch_normalization_2[1][0]      \n","__________________________________________________________________________________________________\n","conv1d (Conv1D)                 (None, 100, 250)     187750      p_re_lu_2[0][0]                  \n","                                                                 p_re_lu_2[1][0]                  \n","__________________________________________________________________________________________________\n","add (Add)                       (None, 100, 250)     0           dropout[0][0]                    \n","                                                                 conv1d[1][0]                     \n","__________________________________________________________________________________________________\n","pool_1 (MaxPooling1D)           (None, 50, 250)      0           add[0][0]                        \n","__________________________________________________________________________________________________\n","batch_normalization_3 (BatchNor (None, 50, 250)      1000        pool_1[0][0]                     \n","                                                                 conv1d_1[0][0]                   \n","__________________________________________________________________________________________________\n","p_re_lu_3 (PReLU)               (None, 50, 250)      12500       batch_normalization_3[0][0]      \n","                                                                 batch_normalization_3[1][0]      \n","__________________________________________________________________________________________________\n","conv1d_1 (Conv1D)               (None, 50, 250)      187750      p_re_lu_3[0][0]                  \n","                                                                 p_re_lu_3[1][0]                  \n","__________________________________________________________________________________________________\n","add_1 (Add)                     (None, 50, 250)      0           pool_1[0][0]                     \n","                                                                 conv1d_1[1][0]                   \n","__________________________________________________________________________________________________\n","pool_2 (MaxPooling1D)           (None, 25, 250)      0           add_1[0][0]                      \n","__________________________________________________________________________________________________\n","batch_normalization_4 (BatchNor (None, 25, 250)      1000        pool_2[0][0]                     \n","                                                                 conv1d_2[0][0]                   \n","__________________________________________________________________________________________________\n","p_re_lu_4 (PReLU)               (None, 25, 250)      6250        batch_normalization_4[0][0]      \n","                                                                 batch_normalization_4[1][0]      \n","__________________________________________________________________________________________________\n","conv1d_2 (Conv1D)               (None, 25, 250)      187750      p_re_lu_4[0][0]                  \n","                                                                 p_re_lu_4[1][0]                  \n","__________________________________________________________________________________________________\n","add_2 (Add)                     (None, 25, 250)      0           pool_2[0][0]                     \n","                                                                 conv1d_2[1][0]                   \n","__________________________________________________________________________________________________\n","pool_3 (MaxPooling1D)           (None, 13, 250)      0           add_2[0][0]                      \n","__________________________________________________________________________________________________\n","batch_normalization_5 (BatchNor (None, 13, 250)      1000        pool_3[0][0]                     \n","                                                                 conv1d_3[0][0]                   \n","__________________________________________________________________________________________________\n","p_re_lu_5 (PReLU)               (None, 13, 250)      3250        batch_normalization_5[0][0]      \n","                                                                 batch_normalization_5[1][0]      \n","__________________________________________________________________________________________________\n","conv1d_3 (Conv1D)               (None, 13, 250)      187750      p_re_lu_5[0][0]                  \n","                                                                 p_re_lu_5[1][0]                  \n","__________________________________________________________________________________________________\n","add_3 (Add)                     (None, 13, 250)      0           pool_3[0][0]                     \n","                                                                 conv1d_3[1][0]                   \n","__________________________________________________________________________________________________\n","pool_4 (MaxPooling1D)           (None, 7, 250)       0           add_3[0][0]                      \n","__________________________________________________________________________________________________\n","batch_normalization_6 (BatchNor (None, 7, 250)       1000        pool_4[0][0]                     \n","                                                                 conv1d_4[0][0]                   \n","__________________________________________________________________________________________________\n","p_re_lu_6 (PReLU)               (None, 7, 250)       1750        batch_normalization_6[0][0]      \n","                                                                 batch_normalization_6[1][0]      \n","__________________________________________________________________________________________________\n","conv1d_4 (Conv1D)               (None, 7, 250)       187750      p_re_lu_6[0][0]                  \n","                                                                 p_re_lu_6[1][0]                  \n","__________________________________________________________________________________________________\n","add_4 (Add)                     (None, 7, 250)       0           pool_4[0][0]                     \n","                                                                 conv1d_4[1][0]                   \n","__________________________________________________________________________________________________\n","global_max_pooling1d (GlobalMax (None, 250)          0           add_4[0][0]                      \n","__________________________________________________________________________________________________\n","dense_2 (Dense)                 (None, 256)          64256       global_max_pooling1d[0][0]       \n","__________________________________________________________________________________________________\n","batch_normalization_1 (BatchNor (None, 256)          1024        dense_2[0][0]                    \n","__________________________________________________________________________________________________\n","p_re_lu_1 (PReLU)               (None, 256)          256         batch_normalization_1[0][0]      \n","__________________________________________________________________________________________________\n","dropout_1 (Dropout)             (None, 256)          0           p_re_lu_1[0][0]                  \n","__________________________________________________________________________________________________\n","dense_3 (Dense)                 (None, 2)            514         dropout_1[0][0]                  \n","==================================================================================================\n","Total params: 336,264,576\n","Trainable params: 4,153,288\n","Non-trainable params: 332,111,288\n","__________________________________________________________________________________________________\n","Epoch 1/20\n","380/381 [============================>.] - ETA: 0s - loss: 0.6603 - acc: 0.7012Epoch 1/20\n"," 96/381 [======>.......................] - ETA: 1:55 - loss: 0.5644 - acc: 0.7150\n","epoch: 0 precision: 0.757498, recall: 0.715036, f1: 0.713157\n","381/381 [==============================] - 233s 611ms/step - loss: 0.6600 - acc: 0.7010 - val_loss: 0.5644 - val_acc: 0.7150\n","Epoch 2/20\n","380/381 [============================>.] - ETA: 0s - loss: 0.5110 - acc: 0.7696Epoch 1/20\n"," 96/381 [======>.......................] - ETA: 1:47 - loss: 0.4731 - acc: 0.7827\n","epoch: 1 precision: 0.787593, recall: 0.784636, f1: 0.780583\n","381/381 [==============================] - 224s 588ms/step - loss: 0.5108 - acc: 0.7693 - val_loss: 0.4731 - val_acc: 0.7827\n","Epoch 3/20\n","380/381 [============================>.] - ETA: 0s - loss: 0.4874 - acc: 0.7766Epoch 1/20\n"," 96/381 [======>.......................] - ETA: 1:47 - loss: 0.4886 - acc: 0.7754\n","epoch: 2 precision: 0.814188, recall: 0.769534, f1: 0.751462\n","381/381 [==============================] - 226s 592ms/step - loss: 0.4870 - acc: 0.7770 - val_loss: 0.4886 - val_acc: 0.7754\n","Epoch 4/20\n","380/381 [============================>.] - ETA: 0s - loss: 0.4612 - acc: 0.7918Epoch 1/20\n"," 96/381 [======>.......................] - ETA: 1:47 - loss: 0.4562 - acc: 0.7873\n","epoch: 3 precision: 0.803242, recall: 0.785949, f1: 0.776847\n","381/381 [==============================] - 226s 593ms/step - loss: 0.4614 - acc: 0.7915 - val_loss: 0.4562 - val_acc: 0.7873\n","Epoch 5/20\n","380/381 [============================>.] - ETA: 0s - loss: 0.4465 - acc: 0.8044Epoch 1/20\n"," 96/381 [======>.......................] - ETA: 1:47 - loss: 0.4888 - acc: 0.7663\n","epoch: 4 precision: 0.815366, recall: 0.778726, f1: 0.763822\n","381/381 [==============================] - 226s 592ms/step - loss: 0.4463 - acc: 0.8044 - val_loss: 0.4888 - val_acc: 0.7663\n","Epoch 6/20\n","380/381 [============================>.] - ETA: 0s - loss: 0.4236 - acc: 0.8188Epoch 1/20\n"," 96/381 [======>.......................] - ETA: 1:48 - loss: 0.4793 - acc: 0.7728\n","epoch: 5 precision: 0.805578, recall: 0.768877, f1: 0.752644\n","381/381 [==============================] - 227s 595ms/step - loss: 0.4240 - acc: 0.8187 - val_loss: 0.4793 - val_acc: 0.7728\n","Epoch 7/20\n","380/381 [============================>.] - ETA: 0s - loss: 0.4071 - acc: 0.8224Epoch 1/20\n"," 96/381 [======>.......................] - ETA: 1:47 - loss: 0.5320 - acc: 0.7223\n","epoch: 6 precision: 0.739773, recall: 0.728168, f1: 0.729452\n","381/381 [==============================] - 226s 594ms/step - loss: 0.4080 - acc: 0.8220 - val_loss: 0.5320 - val_acc: 0.7223\n","Epoch 8/20\n","380/381 [============================>.] - ETA: 0s - loss: 0.3821 - acc: 0.8355Epoch 1/20\n"," 96/381 [======>.......................] - ETA: 1:47 - loss: 0.4853 - acc: 0.7787\n","epoch: 7 precision: 0.771631, recall: 0.772160, f1: 0.771820\n","381/381 [==============================] - 226s 594ms/step - loss: 0.3822 - acc: 0.8353 - val_loss: 0.4853 - val_acc: 0.7787\n","Epoch 9/20\n","380/381 [============================>.] - ETA: 0s - loss: 0.3533 - acc: 0.8477Epoch 1/20\n"," 96/381 [======>.......................] - ETA: 1:47 - loss: 0.6468 - acc: 0.6842\n","epoch: 8 precision: 0.783564, recall: 0.687459, f1: 0.635398\n","381/381 [==============================] - 226s 593ms/step - loss: 0.3538 - acc: 0.8475 - val_loss: 0.6468 - val_acc: 0.6842\n","Epoch 10/20\n","380/381 [============================>.] - ETA: 0s - loss: 0.3386 - acc: 0.8602Epoch 1/20\n"," 96/381 [======>.......................] - ETA: 1:48 - loss: 0.4785 - acc: 0.7794\n","epoch: 9 precision: 0.797631, recall: 0.780039, f1: 0.770336\n","381/381 [==============================] - 226s 594ms/step - loss: 0.3380 - acc: 0.8604 - val_loss: 0.4785 - val_acc: 0.7794\n","Epoch 11/20\n","380/381 [============================>.] - ETA: 0s - loss: 0.3226 - acc: 0.8663Epoch 1/20\n"," 96/381 [======>.......................] - ETA: 1:47 - loss: 0.5118 - acc: 0.7643\n","epoch: 10 precision: 0.802438, recall: 0.771504, f1: 0.757115\n","381/381 [==============================] - 226s 594ms/step - loss: 0.3228 - acc: 0.8663 - val_loss: 0.5118 - val_acc: 0.7643\n","Epoch 12/20\n","380/381 [============================>.] - ETA: 0s - loss: 0.2978 - acc: 0.8752Epoch 1/20\n"," 96/381 [======>.......................] - ETA: 1:47 - loss: 0.5069 - acc: 0.7617\n","epoch: 11 precision: 0.804903, recall: 0.755089, f1: 0.733399\n","381/381 [==============================] - 226s 594ms/step - loss: 0.2975 - acc: 0.8754 - val_loss: 0.5069 - val_acc: 0.7617\n","Epoch 13/20\n","380/381 [============================>.] - ETA: 0s - loss: 0.2764 - acc: 0.8862Epoch 1/20\n"," 96/381 [======>.......................] - ETA: 1:47 - loss: 0.5487 - acc: 0.7472\n","epoch: 12 precision: 0.791600, recall: 0.754432, f1: 0.735931\n","381/381 [==============================] - 226s 593ms/step - loss: 0.2770 - acc: 0.8860 - val_loss: 0.5487 - val_acc: 0.7472\n","Epoch 14/20\n","380/381 [============================>.] - ETA: 0s - loss: 0.2485 - acc: 0.8992Epoch 1/20\n"," 96/381 [======>.......................] - ETA: 1:47 - loss: 0.6844 - acc: 0.6632\n","epoch: 13 precision: 0.778503, recall: 0.674327, f1: 0.614676\n","381/381 [==============================] - 226s 594ms/step - loss: 0.2487 - acc: 0.8992 - val_loss: 0.6844 - val_acc: 0.6632\n","Epoch 15/20\n","380/381 [============================>.] - ETA: 0s - loss: 0.2263 - acc: 0.9095Epoch 1/20\n"," 96/381 [======>.......................] - ETA: 1:47 - loss: 0.7386 - acc: 0.6612\n","epoch: 14 precision: 0.777960, recall: 0.645437, f1: 0.564189\n","381/381 [==============================] - 226s 594ms/step - loss: 0.2262 - acc: 0.9097 - val_loss: 0.7386 - val_acc: 0.6612\n","Epoch 16/20\n","380/381 [============================>.] - ETA: 0s - loss: 0.2177 - acc: 0.9171Epoch 1/20\n"," 96/381 [======>.......................] - ETA: 1:47 - loss: 0.8029 - acc: 0.6297\n","epoch: 15 precision: 0.756277, recall: 0.631648, f1: 0.542708\n","381/381 [==============================] - 226s 594ms/step - loss: 0.2174 - acc: 0.9172 - val_loss: 0.8029 - val_acc: 0.6297\n","Epoch 17/20\n","380/381 [============================>.] - ETA: 0s - loss: 0.2112 - acc: 0.9220Epoch 1/20\n"," 96/381 [======>.......................] - ETA: 1:47 - loss: 0.5893 - acc: 0.7531\n","epoch: 16 precision: 0.794659, recall: 0.753119, f1: 0.733207\n","381/381 [==============================] - 226s 592ms/step - loss: 0.2117 - acc: 0.9220 - val_loss: 0.5893 - val_acc: 0.7531\n","Epoch 18/20\n","380/381 [============================>.] - ETA: 0s - loss: 0.2061 - acc: 0.9243Epoch 1/20\n"," 95/381 [======>.......................] - ETA: 1:48 - loss: 0.6428 - acc: 0.7319\n","epoch: 17 precision: 0.793090, recall: 0.725542, f1: 0.693009\n","381/381 [==============================] - 226s 592ms/step - loss: 0.2060 - acc: 0.9243 - val_loss: 0.6457 - val_acc: 0.7308\n","Epoch 19/20\n","380/381 [============================>.] - ETA: 0s - loss: 0.1803 - acc: 0.9312Epoch 1/20\n"," 96/381 [======>.......................] - ETA: 1:48 - loss: 1.0451 - acc: 0.5811\n","epoch: 18 precision: 0.759111, recall: 0.582403, f1: 0.441992\n","381/381 [==============================] - 227s 595ms/step - loss: 0.1803 - acc: 0.9312 - val_loss: 1.0451 - val_acc: 0.5811\n","Epoch 20/20\n","380/381 [============================>.] - ETA: 0s - loss: 0.1790 - acc: 0.9296Epoch 1/20\n"," 96/381 [======>.......................] - ETA: 1:47 - loss: 0.8121 - acc: 0.6599\n","epoch: 19 precision: 0.771106, recall: 0.669074, f1: 0.607421\n","381/381 [==============================] - 226s 594ms/step - loss: 0.1792 - acc: 0.9294 - val_loss: 0.8121 - val_acc: 0.6599\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7fccaa096eb8>"]},"metadata":{"tags":[]},"execution_count":43}]},{"cell_type":"code","metadata":{"id":"RySwlLRHmBHk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1596594488469,"user_tz":-480,"elapsed":344438,"user":{"displayName":"dy c","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiwUhiP2JGBJF0ccdzeCMO3DywIn5WL-okr917K=s64","userId":"07027550308680493425"}},"outputId":"2a462232-9cee-42fb-8d35-3ec170ad45f4"},"source":["from tensorflow.python import keras\n","from kashgari.tasks.classification import CNN_LSTM_Model\n","\n","from kashgari.callbacks import EvalCallBack\n","\n","import logging\n","logging.basicConfig(level='DEBUG')\n","\n","model = CNN_LSTM_Model(bert_embed)\n","\n","eval_callback = EvalCallBack(kash_model=model,\n","                             valid_x=test_data_x,\n","                             valid_y=test_data_y,\n","                             step=1)\n","early_stop = keras.callbacks.EarlyStopping(patience=10)\n","reduse_lr_callback = keras.callbacks.ReduceLROnPlateau(factor=0.1, patience=5)\n","# 这是 Kashgari 内置回调函数，会在训练过程计算精确度，召回率和 F1\n","eval_callback = EvalCallBack(kash_model=model,\n","                             valid_x=test_data_x,\n","                             valid_y=test_data_y,\n","                             step=5)\n","\n","model.fit(training_data_x,\n","          training_data_y,\n","          test_data_x,\n","          test_data_y,\n","          batch_size=10,epochs=30,callbacks=[eval_callback,early_stop,reduse_lr_callback])"],"execution_count":32,"outputs":[{"output_type":"stream","text":["Model: \"model_7\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","Input-Token (InputLayer)        [(None, 30)]         0                                            \n","__________________________________________________________________________________________________\n","Input-Segment (InputLayer)      [(None, 30)]         0                                            \n","__________________________________________________________________________________________________\n","Embedding-Token (TokenEmbedding [(None, 30, 768), (3 23440896    Input-Token[0][0]                \n","__________________________________________________________________________________________________\n","Embedding-Segment (Embedding)   (None, 30, 768)      1536        Input-Segment[0][0]              \n","__________________________________________________________________________________________________\n","Embedding-Token-Segment (Add)   (None, 30, 768)      0           Embedding-Token[0][0]            \n","                                                                 Embedding-Segment[0][0]          \n","__________________________________________________________________________________________________\n","Embedding-Position (PositionEmb (None, 30, 768)      23040       Embedding-Token-Segment[0][0]    \n","__________________________________________________________________________________________________\n","Embedding-Dropout (Dropout)     (None, 30, 768)      0           Embedding-Position[0][0]         \n","__________________________________________________________________________________________________\n","Embedding-Norm (LayerNormalizat (None, 30, 768)      1536        Embedding-Dropout[0][0]          \n","__________________________________________________________________________________________________\n","Encoder-1-MultiHeadSelfAttentio (None, 30, 768)      2362368     Embedding-Norm[0][0]             \n","__________________________________________________________________________________________________\n","Encoder-1-MultiHeadSelfAttentio (None, 30, 768)      0           Encoder-1-MultiHeadSelfAttention[\n","__________________________________________________________________________________________________\n","Encoder-1-MultiHeadSelfAttentio (None, 30, 768)      0           Embedding-Norm[0][0]             \n","                                                                 Encoder-1-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-1-MultiHeadSelfAttentio (None, 30, 768)      1536        Encoder-1-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-1-FeedForward (FeedForw (None, 30, 768)      4722432     Encoder-1-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-1-FeedForward-Dropout ( (None, 30, 768)      0           Encoder-1-FeedForward[0][0]      \n","__________________________________________________________________________________________________\n","Encoder-1-FeedForward-Add (Add) (None, 30, 768)      0           Encoder-1-MultiHeadSelfAttention-\n","                                                                 Encoder-1-FeedForward-Dropout[0][\n","__________________________________________________________________________________________________\n","Encoder-1-FeedForward-Norm (Lay (None, 30, 768)      1536        Encoder-1-FeedForward-Add[0][0]  \n","__________________________________________________________________________________________________\n","Encoder-2-MultiHeadSelfAttentio (None, 30, 768)      2362368     Encoder-1-FeedForward-Norm[0][0] \n","__________________________________________________________________________________________________\n","Encoder-2-MultiHeadSelfAttentio (None, 30, 768)      0           Encoder-2-MultiHeadSelfAttention[\n","__________________________________________________________________________________________________\n","Encoder-2-MultiHeadSelfAttentio (None, 30, 768)      0           Encoder-1-FeedForward-Norm[0][0] \n","                                                                 Encoder-2-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-2-MultiHeadSelfAttentio (None, 30, 768)      1536        Encoder-2-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-2-FeedForward (FeedForw (None, 30, 768)      4722432     Encoder-2-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-2-FeedForward-Dropout ( (None, 30, 768)      0           Encoder-2-FeedForward[0][0]      \n","__________________________________________________________________________________________________\n","Encoder-2-FeedForward-Add (Add) (None, 30, 768)      0           Encoder-2-MultiHeadSelfAttention-\n","                                                                 Encoder-2-FeedForward-Dropout[0][\n","__________________________________________________________________________________________________\n","Encoder-2-FeedForward-Norm (Lay (None, 30, 768)      1536        Encoder-2-FeedForward-Add[0][0]  \n","__________________________________________________________________________________________________\n","Encoder-3-MultiHeadSelfAttentio (None, 30, 768)      2362368     Encoder-2-FeedForward-Norm[0][0] \n","__________________________________________________________________________________________________\n","Encoder-3-MultiHeadSelfAttentio (None, 30, 768)      0           Encoder-3-MultiHeadSelfAttention[\n","__________________________________________________________________________________________________\n","Encoder-3-MultiHeadSelfAttentio (None, 30, 768)      0           Encoder-2-FeedForward-Norm[0][0] \n","                                                                 Encoder-3-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-3-MultiHeadSelfAttentio (None, 30, 768)      1536        Encoder-3-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-3-FeedForward (FeedForw (None, 30, 768)      4722432     Encoder-3-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-3-FeedForward-Dropout ( (None, 30, 768)      0           Encoder-3-FeedForward[0][0]      \n","__________________________________________________________________________________________________\n","Encoder-3-FeedForward-Add (Add) (None, 30, 768)      0           Encoder-3-MultiHeadSelfAttention-\n","                                                                 Encoder-3-FeedForward-Dropout[0][\n","__________________________________________________________________________________________________\n","Encoder-3-FeedForward-Norm (Lay (None, 30, 768)      1536        Encoder-3-FeedForward-Add[0][0]  \n","__________________________________________________________________________________________________\n","Encoder-4-MultiHeadSelfAttentio (None, 30, 768)      2362368     Encoder-3-FeedForward-Norm[0][0] \n","__________________________________________________________________________________________________\n","Encoder-4-MultiHeadSelfAttentio (None, 30, 768)      0           Encoder-4-MultiHeadSelfAttention[\n","__________________________________________________________________________________________________\n","Encoder-4-MultiHeadSelfAttentio (None, 30, 768)      0           Encoder-3-FeedForward-Norm[0][0] \n","                                                                 Encoder-4-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-4-MultiHeadSelfAttentio (None, 30, 768)      1536        Encoder-4-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-4-FeedForward (FeedForw (None, 30, 768)      4722432     Encoder-4-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-4-FeedForward-Dropout ( (None, 30, 768)      0           Encoder-4-FeedForward[0][0]      \n","__________________________________________________________________________________________________\n","Encoder-4-FeedForward-Add (Add) (None, 30, 768)      0           Encoder-4-MultiHeadSelfAttention-\n","                                                                 Encoder-4-FeedForward-Dropout[0][\n","__________________________________________________________________________________________________\n","Encoder-4-FeedForward-Norm (Lay (None, 30, 768)      1536        Encoder-4-FeedForward-Add[0][0]  \n","__________________________________________________________________________________________________\n","Encoder-5-MultiHeadSelfAttentio (None, 30, 768)      2362368     Encoder-4-FeedForward-Norm[0][0] \n","__________________________________________________________________________________________________\n","Encoder-5-MultiHeadSelfAttentio (None, 30, 768)      0           Encoder-5-MultiHeadSelfAttention[\n","__________________________________________________________________________________________________\n","Encoder-5-MultiHeadSelfAttentio (None, 30, 768)      0           Encoder-4-FeedForward-Norm[0][0] \n","                                                                 Encoder-5-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-5-MultiHeadSelfAttentio (None, 30, 768)      1536        Encoder-5-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-5-FeedForward (FeedForw (None, 30, 768)      4722432     Encoder-5-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-5-FeedForward-Dropout ( (None, 30, 768)      0           Encoder-5-FeedForward[0][0]      \n","__________________________________________________________________________________________________\n","Encoder-5-FeedForward-Add (Add) (None, 30, 768)      0           Encoder-5-MultiHeadSelfAttention-\n","                                                                 Encoder-5-FeedForward-Dropout[0][\n","__________________________________________________________________________________________________\n","Encoder-5-FeedForward-Norm (Lay (None, 30, 768)      1536        Encoder-5-FeedForward-Add[0][0]  \n","__________________________________________________________________________________________________\n","Encoder-6-MultiHeadSelfAttentio (None, 30, 768)      2362368     Encoder-5-FeedForward-Norm[0][0] \n","__________________________________________________________________________________________________\n","Encoder-6-MultiHeadSelfAttentio (None, 30, 768)      0           Encoder-6-MultiHeadSelfAttention[\n","__________________________________________________________________________________________________\n","Encoder-6-MultiHeadSelfAttentio (None, 30, 768)      0           Encoder-5-FeedForward-Norm[0][0] \n","                                                                 Encoder-6-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-6-MultiHeadSelfAttentio (None, 30, 768)      1536        Encoder-6-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-6-FeedForward (FeedForw (None, 30, 768)      4722432     Encoder-6-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-6-FeedForward-Dropout ( (None, 30, 768)      0           Encoder-6-FeedForward[0][0]      \n","__________________________________________________________________________________________________\n","Encoder-6-FeedForward-Add (Add) (None, 30, 768)      0           Encoder-6-MultiHeadSelfAttention-\n","                                                                 Encoder-6-FeedForward-Dropout[0][\n","__________________________________________________________________________________________________\n","Encoder-6-FeedForward-Norm (Lay (None, 30, 768)      1536        Encoder-6-FeedForward-Add[0][0]  \n","__________________________________________________________________________________________________\n","Encoder-7-MultiHeadSelfAttentio (None, 30, 768)      2362368     Encoder-6-FeedForward-Norm[0][0] \n","__________________________________________________________________________________________________\n","Encoder-7-MultiHeadSelfAttentio (None, 30, 768)      0           Encoder-7-MultiHeadSelfAttention[\n","__________________________________________________________________________________________________\n","Encoder-7-MultiHeadSelfAttentio (None, 30, 768)      0           Encoder-6-FeedForward-Norm[0][0] \n","                                                                 Encoder-7-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-7-MultiHeadSelfAttentio (None, 30, 768)      1536        Encoder-7-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-7-FeedForward (FeedForw (None, 30, 768)      4722432     Encoder-7-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-7-FeedForward-Dropout ( (None, 30, 768)      0           Encoder-7-FeedForward[0][0]      \n","__________________________________________________________________________________________________\n","Encoder-7-FeedForward-Add (Add) (None, 30, 768)      0           Encoder-7-MultiHeadSelfAttention-\n","                                                                 Encoder-7-FeedForward-Dropout[0][\n","__________________________________________________________________________________________________\n","Encoder-7-FeedForward-Norm (Lay (None, 30, 768)      1536        Encoder-7-FeedForward-Add[0][0]  \n","__________________________________________________________________________________________________\n","Encoder-8-MultiHeadSelfAttentio (None, 30, 768)      2362368     Encoder-7-FeedForward-Norm[0][0] \n","__________________________________________________________________________________________________\n","Encoder-8-MultiHeadSelfAttentio (None, 30, 768)      0           Encoder-8-MultiHeadSelfAttention[\n","__________________________________________________________________________________________________\n","Encoder-8-MultiHeadSelfAttentio (None, 30, 768)      0           Encoder-7-FeedForward-Norm[0][0] \n","                                                                 Encoder-8-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-8-MultiHeadSelfAttentio (None, 30, 768)      1536        Encoder-8-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-8-FeedForward (FeedForw (None, 30, 768)      4722432     Encoder-8-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-8-FeedForward-Dropout ( (None, 30, 768)      0           Encoder-8-FeedForward[0][0]      \n","__________________________________________________________________________________________________\n","Encoder-8-FeedForward-Add (Add) (None, 30, 768)      0           Encoder-8-MultiHeadSelfAttention-\n","                                                                 Encoder-8-FeedForward-Dropout[0][\n","__________________________________________________________________________________________________\n","Encoder-8-FeedForward-Norm (Lay (None, 30, 768)      1536        Encoder-8-FeedForward-Add[0][0]  \n","__________________________________________________________________________________________________\n","Encoder-9-MultiHeadSelfAttentio (None, 30, 768)      2362368     Encoder-8-FeedForward-Norm[0][0] \n","__________________________________________________________________________________________________\n","Encoder-9-MultiHeadSelfAttentio (None, 30, 768)      0           Encoder-9-MultiHeadSelfAttention[\n","__________________________________________________________________________________________________\n","Encoder-9-MultiHeadSelfAttentio (None, 30, 768)      0           Encoder-8-FeedForward-Norm[0][0] \n","                                                                 Encoder-9-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-9-MultiHeadSelfAttentio (None, 30, 768)      1536        Encoder-9-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-9-FeedForward (FeedForw (None, 30, 768)      4722432     Encoder-9-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-9-FeedForward-Dropout ( (None, 30, 768)      0           Encoder-9-FeedForward[0][0]      \n","__________________________________________________________________________________________________\n","Encoder-9-FeedForward-Add (Add) (None, 30, 768)      0           Encoder-9-MultiHeadSelfAttention-\n","                                                                 Encoder-9-FeedForward-Dropout[0][\n","__________________________________________________________________________________________________\n","Encoder-9-FeedForward-Norm (Lay (None, 30, 768)      1536        Encoder-9-FeedForward-Add[0][0]  \n","__________________________________________________________________________________________________\n","Encoder-10-MultiHeadSelfAttenti (None, 30, 768)      2362368     Encoder-9-FeedForward-Norm[0][0] \n","__________________________________________________________________________________________________\n","Encoder-10-MultiHeadSelfAttenti (None, 30, 768)      0           Encoder-10-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-10-MultiHeadSelfAttenti (None, 30, 768)      0           Encoder-9-FeedForward-Norm[0][0] \n","                                                                 Encoder-10-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-10-MultiHeadSelfAttenti (None, 30, 768)      1536        Encoder-10-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-10-FeedForward (FeedFor (None, 30, 768)      4722432     Encoder-10-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-10-FeedForward-Dropout  (None, 30, 768)      0           Encoder-10-FeedForward[0][0]     \n","__________________________________________________________________________________________________\n","Encoder-10-FeedForward-Add (Add (None, 30, 768)      0           Encoder-10-MultiHeadSelfAttention\n","                                                                 Encoder-10-FeedForward-Dropout[0]\n","__________________________________________________________________________________________________\n","Encoder-10-FeedForward-Norm (La (None, 30, 768)      1536        Encoder-10-FeedForward-Add[0][0] \n","__________________________________________________________________________________________________\n","Encoder-11-MultiHeadSelfAttenti (None, 30, 768)      2362368     Encoder-10-FeedForward-Norm[0][0]\n","__________________________________________________________________________________________________\n","Encoder-11-MultiHeadSelfAttenti (None, 30, 768)      0           Encoder-11-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-11-MultiHeadSelfAttenti (None, 30, 768)      0           Encoder-10-FeedForward-Norm[0][0]\n","                                                                 Encoder-11-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-11-MultiHeadSelfAttenti (None, 30, 768)      1536        Encoder-11-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-11-FeedForward (FeedFor (None, 30, 768)      4722432     Encoder-11-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-11-FeedForward-Dropout  (None, 30, 768)      0           Encoder-11-FeedForward[0][0]     \n","__________________________________________________________________________________________________\n","Encoder-11-FeedForward-Add (Add (None, 30, 768)      0           Encoder-11-MultiHeadSelfAttention\n","                                                                 Encoder-11-FeedForward-Dropout[0]\n","__________________________________________________________________________________________________\n","Encoder-11-FeedForward-Norm (La (None, 30, 768)      1536        Encoder-11-FeedForward-Add[0][0] \n","__________________________________________________________________________________________________\n","Encoder-12-MultiHeadSelfAttenti (None, 30, 768)      2362368     Encoder-11-FeedForward-Norm[0][0]\n","__________________________________________________________________________________________________\n","Encoder-12-MultiHeadSelfAttenti (None, 30, 768)      0           Encoder-12-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-12-MultiHeadSelfAttenti (None, 30, 768)      0           Encoder-11-FeedForward-Norm[0][0]\n","                                                                 Encoder-12-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-12-MultiHeadSelfAttenti (None, 30, 768)      1536        Encoder-12-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-12-FeedForward (FeedFor (None, 30, 768)      4722432     Encoder-12-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-12-FeedForward-Dropout  (None, 30, 768)      0           Encoder-12-FeedForward[0][0]     \n","__________________________________________________________________________________________________\n","Encoder-12-FeedForward-Add (Add (None, 30, 768)      0           Encoder-12-MultiHeadSelfAttention\n","                                                                 Encoder-12-FeedForward-Dropout[0]\n","__________________________________________________________________________________________________\n","Encoder-12-FeedForward-Norm (La (None, 30, 768)      1536        Encoder-12-FeedForward-Add[0][0] \n","__________________________________________________________________________________________________\n","Encoder-Output (Concatenate)    (None, 30, 3072)     0           Encoder-9-FeedForward-Norm[0][0] \n","                                                                 Encoder-10-FeedForward-Norm[0][0]\n","                                                                 Encoder-11-FeedForward-Norm[0][0]\n","                                                                 Encoder-12-FeedForward-Norm[0][0]\n","__________________________________________________________________________________________________\n","non_masking_layer (NonMaskingLa (None, 30, 3072)     0           Encoder-Output[0][0]             \n","__________________________________________________________________________________________________\n","conv1d_1 (Conv1D)               (None, 30, 32)       294944      non_masking_layer[0][0]          \n","__________________________________________________________________________________________________\n","max_pooling1d_1 (MaxPooling1D)  (None, 15, 32)       0           conv1d_1[0][0]                   \n","__________________________________________________________________________________________________\n","cu_dnnlstm_2 (CuDNNLSTM)        (None, 100)          53600       max_pooling1d_1[0][0]            \n","__________________________________________________________________________________________________\n","dense_2 (Dense)                 (None, 2)            202         cu_dnnlstm_2[0][0]               \n","==================================================================================================\n","Total params: 108,870,218\n","Trainable params: 348,746\n","Non-trainable params: 108,521,472\n","__________________________________________________________________________________________________\n","Epoch 1/30\n","609/610 [============================>.] - ETA: 0s - loss: 0.4985 - acc: 0.7681Epoch 1/30\n","610/610 [==============================] - 27s 44ms/step - loss: 0.4989 - acc: 0.7679 - val_loss: 0.4381 - val_acc: 0.8096\n","Epoch 2/30\n","609/610 [============================>.] - ETA: 0s - loss: 0.4384 - acc: 0.8089Epoch 1/30\n","610/610 [==============================] - 20s 33ms/step - loss: 0.4389 - acc: 0.8085 - val_loss: 0.4463 - val_acc: 0.7984\n","Epoch 3/30\n","609/610 [============================>.] - ETA: 0s - loss: 0.4172 - acc: 0.8199Epoch 1/30\n","610/610 [==============================] - 20s 34ms/step - loss: 0.4166 - acc: 0.8202 - val_loss: 0.4684 - val_acc: 0.7912\n","Epoch 4/30\n","609/610 [============================>.] - ETA: 0s - loss: 0.3939 - acc: 0.8304Epoch 1/30\n","610/610 [==============================] - 20s 34ms/step - loss: 0.3936 - acc: 0.8305 - val_loss: 0.4360 - val_acc: 0.8194\n","Epoch 5/30\n","609/610 [============================>.] - ETA: 0s - loss: 0.3745 - acc: 0.8419Epoch 1/30\n","153/610 [======>.......................] - ETA: 11s - loss: 0.4743 - acc: 0.8011\n","epoch: 4 precision: 0.822256, recall: 0.803677, f1: 0.795636\n","610/610 [==============================] - 26s 43ms/step - loss: 0.3747 - acc: 0.8416 - val_loss: 0.4743 - val_acc: 0.8011\n","Epoch 6/30\n","609/610 [============================>.] - ETA: 0s - loss: 0.3474 - acc: 0.8540Epoch 1/30\n","610/610 [==============================] - 20s 33ms/step - loss: 0.3481 - acc: 0.8538 - val_loss: 0.4181 - val_acc: 0.8188\n","Epoch 7/30\n","609/610 [============================>.] - ETA: 0s - loss: 0.3188 - acc: 0.8685Epoch 1/30\n","610/610 [==============================] - 20s 33ms/step - loss: 0.3186 - acc: 0.8685 - val_loss: 0.4668 - val_acc: 0.8004\n","Epoch 8/30\n","609/610 [============================>.] - ETA: 0s - loss: 0.2965 - acc: 0.8801Epoch 1/30\n","610/610 [==============================] - 20s 33ms/step - loss: 0.2965 - acc: 0.8800 - val_loss: 0.5286 - val_acc: 0.7754\n","Epoch 9/30\n","609/610 [============================>.] - ETA: 0s - loss: 0.2744 - acc: 0.8885Epoch 1/30\n","610/610 [==============================] - 20s 33ms/step - loss: 0.2748 - acc: 0.8884 - val_loss: 0.5112 - val_acc: 0.7965\n","Epoch 10/30\n","609/610 [============================>.] - ETA: 0s - loss: 0.2519 - acc: 0.8956Epoch 1/30\n","152/610 [======>.......................] - ETA: 11s - loss: 0.5516 - acc: 0.7892\n","epoch: 9 precision: 0.780610, recall: 0.781353, f1: 0.780732\n","610/610 [==============================] - 22s 36ms/step - loss: 0.2518 - acc: 0.8956 - val_loss: 0.5509 - val_acc: 0.7899\n","Epoch 11/30\n","609/610 [============================>.] - ETA: 0s - loss: 0.2310 - acc: 0.9064Epoch 1/30\n","610/610 [==============================] - 21s 34ms/step - loss: 0.2309 - acc: 0.9064 - val_loss: 0.5828 - val_acc: 0.7866\n","Epoch 12/30\n","609/610 [============================>.] - ETA: 0s - loss: 0.1790 - acc: 0.9279Epoch 1/30\n","610/610 [==============================] - 20s 34ms/step - loss: 0.1788 - acc: 0.9280 - val_loss: 0.6200 - val_acc: 0.7951\n","Epoch 13/30\n","609/610 [============================>.] - ETA: 0s - loss: 0.1682 - acc: 0.9338Epoch 1/30\n","610/610 [==============================] - 20s 33ms/step - loss: 0.1682 - acc: 0.9338 - val_loss: 0.6665 - val_acc: 0.7853\n","Epoch 14/30\n","609/610 [============================>.] - ETA: 0s - loss: 0.1606 - acc: 0.9371Epoch 1/30\n","610/610 [==============================] - 20s 33ms/step - loss: 0.1608 - acc: 0.9369 - val_loss: 0.6437 - val_acc: 0.7879\n","Epoch 15/30\n","609/610 [============================>.] - ETA: 0s - loss: 0.1526 - acc: 0.9417Epoch 1/30\n","151/610 [======>.......................] - ETA: 11s - loss: 0.6607 - acc: 0.7914\n","epoch: 14 precision: 0.789649, recall: 0.789888, f1: 0.788044\n","610/610 [==============================] - 22s 36ms/step - loss: 0.1524 - acc: 0.9418 - val_loss: 0.6624 - val_acc: 0.7912\n","Epoch 16/30\n","609/610 [============================>.] - ETA: 0s - loss: 0.1449 - acc: 0.9430Epoch 1/30\n","610/610 [==============================] - 20s 33ms/step - loss: 0.1447 - acc: 0.9431 - val_loss: 0.6902 - val_acc: 0.7866\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f9542a7e518>"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"id":"G2xPazkqn9dE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":166},"executionInfo":{"status":"ok","timestamp":1596594548409,"user_tz":-480,"elapsed":6509,"user":{"displayName":"dy c","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiwUhiP2JGBJF0ccdzeCMO3DywIn5WL-okr917K=s64","userId":"07027550308680493425"}},"outputId":"7ee9dbfb-7081-4d80-88a6-bae2f8f3f647"},"source":["model.evaluate(test_data_x,test_data_y,debug_info=True)"],"execution_count":33,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0     0.7946    0.8478    0.8203       867\n","           1     0.7793    0.7104    0.7432       656\n","\n","    accuracy                         0.7886      1523\n","   macro avg     0.7869    0.7791    0.7818      1523\n","weighted avg     0.7880    0.7886    0.7871      1523\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Z195yyOOrJv4","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596594601732,"user_tz":-480,"elapsed":1077,"user":{"displayName":"dy c","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiwUhiP2JGBJF0ccdzeCMO3DywIn5WL-okr917K=s64","userId":"07027550308680493425"}}},"source":["df=pd.read_csv(r\"/content/drive/My Drive/KAGGLE/nlp/twitter/data/test.csv\")"],"execution_count":34,"outputs":[]},{"cell_type":"code","metadata":{"id":"nOqtX93umQFG","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596594603753,"user_tz":-480,"elapsed":1217,"user":{"displayName":"dy c","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiwUhiP2JGBJF0ccdzeCMO3DywIn5WL-okr917K=s64","userId":"07027550308680493425"}}},"source":["df=df.fillna(\" \")"],"execution_count":35,"outputs":[]},{"cell_type":"code","metadata":{"id":"-GESusFjmV6S","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596594604172,"user_tz":-480,"elapsed":1284,"user":{"displayName":"dy c","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiwUhiP2JGBJF0ccdzeCMO3DywIn5WL-okr917K=s64","userId":"07027550308680493425"}}},"source":["cleaned_pre=df[\"text\"].apply(pre)"],"execution_count":36,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZxmTBryQmiK9","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596594604173,"user_tz":-480,"elapsed":969,"user":{"displayName":"dy c","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiwUhiP2JGBJF0ccdzeCMO3DywIn5WL-okr917K=s64","userId":"07027550308680493425"}}},"source":["pred_new_fe=\"$ \"+df['keyword']+\" $ \"+df[\"location\"]+\" $ \"+cleaned_pre"],"execution_count":37,"outputs":[]},{"cell_type":"code","metadata":{"id":"2G7d-G__nERH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":216},"executionInfo":{"status":"ok","timestamp":1596594604598,"user_tz":-480,"elapsed":982,"user":{"displayName":"dy c","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiwUhiP2JGBJF0ccdzeCMO3DywIn5WL-okr917K=s64","userId":"07027550308680493425"}},"outputId":"05874f71-8fd1-462b-b005-413a11bf4f08"},"source":["pred_new_fe.head(100)"],"execution_count":38,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0          $   $   $ just happened a terrible car crash\n","1     $   $   $ heard about earthquake is different ...\n","2     $   $   $ there is a forest fire at spot pond ...\n","3       $   $   $ apocalypse lighting spokane wildfires\n","4     $   $   $ typhoon soudelor kills  in china and...\n","                            ...                        \n","95    $ annihilated $   $ if your nature appropriate...\n","96    $ annihilated $   $ ninahoag  if you shred my ...\n","97    $ annihilated $ upstate NY $ thehill this is  ...\n","98    $ annihilated $   $ aug  ûókaiserjaegers wipe...\n","99    $ annihilated $   $ they should all die all of...\n","Length: 100, dtype: object"]},"metadata":{"tags":[]},"execution_count":38}]},{"cell_type":"code","metadata":{"id":"mHWm4CyrnUCy","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596594605204,"user_tz":-480,"elapsed":1151,"user":{"displayName":"dy c","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiwUhiP2JGBJF0ccdzeCMO3DywIn5WL-okr917K=s64","userId":"07027550308680493425"}}},"source":["res=pred_new_fe.apply(lambda x: x.split())\n","res=res.tolist()"],"execution_count":39,"outputs":[]},{"cell_type":"code","metadata":{"id":"OXDbjtIH8gJj","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596594613778,"user_tz":-480,"elapsed":7382,"user":{"displayName":"dy c","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiwUhiP2JGBJF0ccdzeCMO3DywIn5WL-okr917K=s64","userId":"07027550308680493425"}}},"source":["tested_y=model.predict(res)"],"execution_count":40,"outputs":[]},{"cell_type":"code","metadata":{"id":"URWvzuWg8zV_","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596594613781,"user_tz":-480,"elapsed":7116,"user":{"displayName":"dy c","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiwUhiP2JGBJF0ccdzeCMO3DywIn5WL-okr917K=s64","userId":"07027550308680493425"}}},"source":["df_fin=pd.DataFrame(zip(df['id'].tolist(),tested_y),columns=['id','target'])"],"execution_count":41,"outputs":[]},{"cell_type":"code","metadata":{"id":"9cAWNIYM9b5f","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596594616277,"user_tz":-480,"elapsed":959,"user":{"displayName":"dy c","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiwUhiP2JGBJF0ccdzeCMO3DywIn5WL-okr917K=s64","userId":"07027550308680493425"}}},"source":["df_fin.to_csv(r'/content/drive/My Drive/KAGGLE/nlp/twitter/res.csv')"],"execution_count":42,"outputs":[]},{"cell_type":"code","metadata":{"id":"jD3cMFMz99BK","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}